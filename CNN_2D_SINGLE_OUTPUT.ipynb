{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5da1058",
   "metadata": {},
   "source": [
    "# DeepRegQA - CNN for predicting quality metrics for image registrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3d1e2",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3177d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b236d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:39.766443Z",
     "start_time": "2022-05-25T22:14:37.973875Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579618e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:40.048678Z",
     "start_time": "2022-05-25T22:14:39.768648Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Important to assign the processing to the GPU with the most vRAM available\n",
    "GPU = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU) #\"1\"\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(device_count = {'GPU': GPU})\n",
    "\n",
    "#Important to run this in order to not overload the GPU\n",
    "config = tf.compat.v1.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.9 # don't hog all vRAM\n",
    "#config.operation_timeout_in_ms=15000   # terminate on long hangs\n",
    "#config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(\"\", config=config)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15934c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:40.728352Z",
     "start_time": "2022-05-25T22:14:40.053245Z"
    }
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "# import xarray as xr\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#import tqdm.keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c2766",
   "metadata": {},
   "source": [
    "### Load Previous Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026fe28",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12a171",
   "metadata": {},
   "source": [
    "Use command %reset to delete all variables in case you want to clear up and load from new\n",
    "\n",
    "[Magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-reset): Calling `%reset -f` after defining variables removes all variables from memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f625b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:40.744443Z",
     "start_time": "2022-05-25T22:14:40.731307Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'BRAIN_STEM', 'SPINAL_CORD', 'PAROTID_RT', 'PAROTID_LT' \n",
    "organ = 'PAROTID_LT'\n",
    "\n",
    "# folder = '/hepgpu9-data1/sbridger/mphys_2d_network_2021-22/slice_arrays/NEW_DATA/'+organ+'/complete_arrays/'\\\n",
    "# + 'COMPLETE_' + organ + '_'\n",
    "\n",
    "train_test = 'test_even' #'test_even' train_odd\n",
    "\n",
    "folder = '/hepgpu9-data1/sbridger/mphys_2d_network_2021-22/slice_arrays/new_registrations/{}/complete_arrays/{}_{}_'\\\n",
    ".format(organ, train_test, organ)\n",
    "\n",
    "complete_img_name = folder+'multiple_patient_image_dvf_array.npy'\n",
    "complete_dice_name = folder+'multiple_dice_array.npy'\n",
    "complete_hd_95_name = folder+'multiple_hd_95_array.npy'\n",
    "complete_hd_75_name = folder+'multiple_hd_75_array.npy'\n",
    "complete_hd_50_name = folder+'multiple_hd_50_array.npy'\n",
    "complete_rms_name = folder+'multiple_rms_array.npy'\n",
    "complete_msd_name = folder+'multiple_msd_array.npy'\n",
    "\n",
    "multiple_patient_image_array = np.load(complete_img_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_dice_array = np.load(complete_dice_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_hd_95_array = np.load(complete_hd_95_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_hd_75_array = np.load(complete_hd_75_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_hd_50_array = np.load(complete_hd_50_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_rms_array = np.load(complete_rms_name, mmap_mode = 'r')\n",
    "\n",
    "multiple_msd_array = np.load(complete_msd_name, mmap_mode = 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15588b80",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cc864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:46.206294Z",
     "start_time": "2022-05-25T22:14:46.202735Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"dice\": multiple_dice_array,\n",
    "    \"hd_95\": multiple_hd_95_array,\n",
    "    \"hd_75\": multiple_hd_75_array,\n",
    "    \"hd_50\": multiple_hd_50_array,\n",
    "    \"msd\": multiple_msd_array,\n",
    "    \"rms\": multiple_rms_array,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe64e6b",
   "metadata": {},
   "source": [
    "Choose a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53201cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:46.778197Z",
     "start_time": "2022-05-25T22:14:46.208375Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = 'dice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac545fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:47.191349Z",
     "start_time": "2022-05-25T22:14:46.781744Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_metric_array = metrics[metric]\n",
    "    print(\"You have chosen: \", metric)\n",
    "    #print(raw_metric_array)\n",
    "except:\n",
    "    print(\"Please choose a valid metric in the above cell (dice, hd_95, hd_75, hd_50, rms or msd)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b26fe",
   "metadata": {},
   "source": [
    "**Sort/trim the data**\n",
    "- To remove all zero DICE scores, without sorting use: `metric_array = raw_metric_array[np.where(multiple_dice_array != 0)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e4d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:47.710341Z",
     "start_time": "2022-05-25T22:14:47.197558Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalise(array):\n",
    "    min_value = np.min(array)\n",
    "    max_value = np.max(array)\n",
    "    n_array = (array-min_value)/(max_value-min_value)\n",
    "    \n",
    "    return n_array, min_value, max_value\n",
    "\n",
    "def transformed_array(array, power, metric_name):\n",
    "    print(\"Working with \"+metric_name+\":\")\n",
    "#     array = raw_metric_array\n",
    "    print(\"Original min: {}    max:  {}\".format(np.min(array),np.max(array)))\n",
    "    \n",
    "    if power!=1:\n",
    "        array = array**(1/power)\n",
    "        #2: Normalise metric aray\n",
    "        array, min_value, max_value = normalise(array)\n",
    "        print(\"Values after taking the {}-root and normalising: \".format(power))\n",
    "        print(\"Min value: \", min_value, \"\\t Max value: \", max_value)    \n",
    "    #     Repeat operation\n",
    "        array = array**(1/power)\n",
    "        array, min_value, max_value = normalise(array)\n",
    "        print(\"Values after taking the {}th root and normalising: \".format(power**2))\n",
    "        \n",
    "    else:\n",
    "        array, min_value, max_value = normalise(array)\n",
    "        \n",
    "    print(\"After normalising:\")\n",
    "    print(\"Min value: \", np.min(array), \"\\t Max value: \", np.max(array))\n",
    "    print(\"Mean value: \", np.mean(array))\n",
    "    \n",
    "    return array\n",
    "\n",
    "def transform_dice_array(array, metric_name):\n",
    "#     array = raw_metric_array\n",
    "\n",
    "    print(\"Working with \"+metric_name+\":\")\n",
    "    print(\"Max value no zero Dice Score: \\t\",np.max(array))\n",
    "    print(\"Min value no zero Dice Score: \\t\",np.min(array))\n",
    "    print(\"Mean value no zero Dice Score: \\t\",np.mean(array))\n",
    "\n",
    "    zeros=0\n",
    "    for i in array:\n",
    "        if i == 0:\n",
    "            zeros+=1\n",
    "\n",
    "    print(\"Number of zero dice scores: \", str(zeros))\n",
    "    array, min_value, max_value = normalise(array)\n",
    "\n",
    "    print(\"After normalising:\")\n",
    "    print(\"Min value: \", np.min(array), \"\\t Max value: \", np.max(array))\n",
    "    print(\"Mean value: \", np.mean(array))\n",
    "\n",
    "    return array\n",
    "    \n",
    "if metric == \"rms\" or metric == 'msd':\n",
    "    metric_array = transformed_array(raw_metric_array, 2, metric)\n",
    "elif metric == \"dice\": \n",
    "    metric_array = transform_dice_array(raw_metric_array, metric)\n",
    "else:\n",
    "    metric_array = transformed_array(raw_metric_array, 1, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e1e98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:48.172365Z",
     "start_time": "2022-05-25T22:14:47.713203Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Our chosen metric is: \", metric)\n",
    "min_value = np.min(metric_array)\n",
    "max_value = np.max(metric_array)\n",
    "print(\"Min value: \", min_value, \"\\t Max value: \", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2be9fc",
   "metadata": {},
   "source": [
    "### Weight Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55a866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:48.579256Z",
     "start_time": "2022-05-25T22:14:48.176794Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3e6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:49.636857Z",
     "start_time": "2022-05-25T22:14:48.583589Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "import seaborn as sb # A data visualisation library based on matplotlib\n",
    "import datetime #module to provide the current date and time for use in the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03280ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:49.829898Z",
     "start_time": "2022-05-25T22:14:49.638532Z"
    }
   },
   "outputs": [],
   "source": [
    "#Produce histogram for the frequency of each value of the metric. \n",
    "\n",
    "n_bins = 40\n",
    "n_actual_metric_hist = np.zeros(n_bins)\n",
    "n_actual_metric_hist, bins_actual_metric_hist, patches_actual_metric_hist = \\\n",
    "    plt.hist(metric_array, density = False, bins = n_bins)#, range = (0,1))\n",
    "#The greater the number of bins the more localised they are in each of the Dice Scores.\n",
    "#With smaller number of bins, a bigger range of Dice Scores would correspond to the same bin (ie. lower \"resolution\")\n",
    "# print(bins_actual_metric_hist)\n",
    "# print(n_actual_metric_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c1cc5",
   "metadata": {},
   "source": [
    "##### Create Array of Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0ce07",
   "metadata": {},
   "source": [
    "Each bin has a weight = 1/frequency and each value in that bin has that weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323cc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:50.309475Z",
     "start_time": "2022-05-25T22:14:49.832008Z"
    }
   },
   "outputs": [],
   "source": [
    "def binningWeights(data, nbins, weight_offset):\n",
    "    n, bins = np.histogram(data, nbins)\n",
    "    weights=[]; j=0; bin_edges=[]\n",
    "    for x in data: # loop through data\n",
    "        i=0\n",
    "        while i in range(len(bins)): # loop through bins\n",
    "#             if x==bins[i]: \n",
    "#                 bin_edges.append(x)\n",
    "            if bins[i] < x <= bins[i+1]: # if data point is in bin\n",
    "                j=i # save index\n",
    "                i=None # exit while loop\n",
    "            else:\n",
    "                i=i+1\n",
    "#         if j==0:\n",
    "#             weights.append(1/(n[j]+weight_offset*weight_factor)) # ensure low values have less/ equal weighted freq.  \n",
    "        else:\n",
    "            weights.append(1/(n[j]+weight_offset)) # assign weight as 1/N for bin j\n",
    "#     print(len(bin_edges))\n",
    "    weight_array = np.array(weights)\n",
    "    return weight_array\n",
    "\n",
    "def binning2DWeights(data_x, data_y, nbins, weight_offset):\n",
    "    n, bins_x, bins_y = np.histogram2d(data_x, data_y, nbins)\n",
    "#     print(n.shape)\n",
    "    weights=[]\n",
    "    j_y = 0; j_x = 0\n",
    "    for x, y in zip(data_x, data_y): # loop through data\n",
    "        # find x bin\n",
    "        i_x=0\n",
    "        while i_x in range(len(bins_x)): # loop through bins\n",
    "            if bins_x[i_x] <= x <= bins_x[i_x+1]: # if data point is in bin\n",
    "                j_x=i_x # save index\n",
    "                i_x=None # exit while loop\n",
    "            else:\n",
    "                i_x=i_x+1 \n",
    "                \n",
    "        # find y bin\n",
    "        i_y=0\n",
    "        while i_y in range(len(bins_y)): # loop through bins\n",
    "            if bins_y[i_y] <= y <= bins_y[i_y+1]: # if data point is in bin\n",
    "                j_y=i_y # save index\n",
    "                i_y=None # exit while loop\n",
    "            else:\n",
    "                i_y=i_y+1 \n",
    "        \n",
    "        weights.append(1/(n[j_x,j_y]+weight_offset)) # assign weight as 1/N for bin j\n",
    "#         print(1/(n[j_x,j_y]+weight_offset))\n",
    "    weights_arr = np.array(weights)\n",
    "#     print(weights_arr.shape)\n",
    "    return weights_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86942084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:51.586146Z",
     "start_time": "2022-05-25T22:14:50.313745Z"
    }
   },
   "outputs": [],
   "source": [
    "weightArray = binningWeights(metric_array, n_bins, 3)\n",
    "\n",
    "# print(np.mean(weightArray))\n",
    "weightArray /= np.mean(weightArray)\n",
    "# print(np.mean(weightArray))\n",
    "plt.plot(weightArray)\n",
    "plt.show()\n",
    "print(\"Mean ± std weight: {:.2f} ± {:.2f}\".format(np.mean(weightArray), np.std(weightArray)))\n",
    "print(\"Min weight {:.3f}:\".format(np.min(weightArray)))\n",
    "print(\"Max weight {:.2f}:\".format(np.max(weightArray)))\n",
    "\n",
    "# weightArray = weightArray/np.mean(weightArray)\n",
    "\n",
    "metric_name = metric\n",
    "fig, ax1 = plt.subplots(figsize=(5, 3.5))\n",
    "\n",
    "#removes the grey background to the plot\n",
    "fig.patch.set_facecolor('white')\n",
    "N, Bins, Patches = plt.hist(metric_array, weights = weightArray, density = False, bins = n_bins, range = (0,1))\n",
    "ax1.set_ylabel('Weighted Frequency', fontsize=18)\n",
    "ax1.set_xlabel(metric_name, fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "totalNumberSlices = str(metric_array.shape[0])\n",
    "plt.tight_layout()\n",
    "#plt.savefig(folder_arrays_load+'/frequency_histograms/'+metric+'/raw_'+metric+'_histogram_' + organ + '_' + totalNumberSlices + '_fourth_root.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean ± std weighted frequency: {:.1f} ± {:.1f}\".format(np.mean(N), np.std(N)))\n",
    "print(\"Min weighted frequency: {:.1f}\".format(np.min(N)))\n",
    "print(\"Max weighted frequency: {:.1f}\".format(np.max(N)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14bacf",
   "metadata": {},
   "source": [
    "## Construct the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bf7bf",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41f17a",
   "metadata": {},
   "source": [
    "* https://keras.io/\n",
    "* https://keras.io/getting-started/sequential-model-guide/ Guide to sequential model (with some examples)\n",
    "* https://github.com/keras-team/keras/tree/master/examples More examples\n",
    "* https://keras.io/models/sequential/ Methods of the sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4fce2",
   "metadata": {},
   "source": [
    "**Importing tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4013e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:51.619821Z",
     "start_time": "2022-05-25T22:14:51.588925Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense, Activation,\\\n",
    "GaussianNoise, Input, concatenate, Reshape, GlobalAveragePooling2D, AveragePooling2D, UpSampling2D, ZeroPadding2D, Add\n",
    "from tensorflow.keras.layers import LeakyReLU, ELU , ReLU\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras import initializers\n",
    "#tf.keras.mixed_precision.experimental.set_policy('mixed_float16') # Mixed precision should increase processing speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345af8e",
   "metadata": {},
   "source": [
    "Below are 4 different models. We will use the `single_functional_feedforward_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360009dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:29:24.467083Z",
     "start_time": "2022-05-25T22:29:24.394084Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some functions\n",
    "def absolute(x):\n",
    "    return abs(x)\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "get_custom_objects().update({'absolute': Activation(absolute)})\n",
    "\n",
    "def relu_max(x):\n",
    "    return relu(x, max_value=1)\n",
    "\n",
    "# The different CNN models\n",
    "def vector_avg_pooling_model():\n",
    "    l2_reg = 0.0005\n",
    "    gn = 0 #0.0005\n",
    "    initializer = tf.keras.initializers.he_normal(seed=1)\n",
    "    img_input = Input(shape=single_input_shape)\n",
    "#     x = GaussianNoise(gn)(img_input)\n",
    "#     x = Reshape((single_input_shape))(x)\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(img_input)\n",
    "    x = Conv2D(4, kernel_size=(3,3), input_shape=single_input_shape, strides=(1, 1), data_format = 'channels_last',\\\n",
    "               kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    out_metric1 = Dense(1, activation='sigmoid', dtype=tf.float32, name=metric1, kernel_initializer=initializer)(x)\n",
    "    out_metric2 = Dense(1, activation='sigmoid', dtype=tf.float32, name=metric2, kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs=img_input, outputs=[out_metric1, out_metric2])\n",
    "\n",
    "    return model\n",
    "\n",
    "def vector_model():\n",
    "    l2_reg = 0\n",
    "    gn = 0 #0.0005\n",
    "    initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.07) #  he_normal(seed=1)\n",
    "    img_input = Input(shape=single_input_shape)\n",
    "#     x = GaussianNoise(gn)(img_input)\n",
    "#     x = Reshape((single_input_shape))(x)\n",
    "    x = BatchNormalization()(img_input)\n",
    "    \n",
    "    x = Conv2D(32, kernel_size=(3,3), input_shape=single_input_shape, strides=(1, 1),\\\n",
    "               kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=(3,3), input_shape=single_input_shape, strides=(1, 1),\\\n",
    "               kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(img_input)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Conv2D(128, kernel_size=(3,3), input_shape=single_input_shape, strides=(2, 2),\\\n",
    "               kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(8)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "    out_metric = Dense(1, activation='sigmoid', dtype=tf.float32, name=metric, kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs=img_input, outputs=[out_metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def vector_model_sequential():\n",
    "    l2_reg = 0\n",
    "    gn = 0 #0.0005\n",
    "    initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.07) #he_normal(seed=1)\n",
    "#     img_input = Input(shape=single_input_shape)\n",
    "#     GaussianNoise(gn)(img_input)\n",
    "#     Reshape((single_input_shape))\n",
    "    lay1,lay2,lay3,dense = 32,64,128,32 #4,8,16,8 #16,32,64,32 # 8,16,32,16\n",
    "    model = Sequential([\n",
    "        \n",
    "        AveragePooling2D(input_shape=single_input_shape),\n",
    "\n",
    "        Conv2D(lay1, kernel_size=(3,3), strides=(1, 1),\\\n",
    "                   kernel_regularizer=l2(l2_reg), kernel_initializer=initializer),\n",
    "        ReLU(),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(lay2, kernel_size=(3,3), strides=(1, 1), data_format = 'channels_last',\\\n",
    "                   kernel_regularizer=l2(l2_reg), kernel_initializer=initializer),\n",
    "        ReLU(),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(lay3, kernel_size=(3,3), strides=(2, 2), data_format = 'channels_last',\\\n",
    "                   kernel_regularizer=l2(l2_reg), kernel_initializer=initializer),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(dense, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "    #     GlobalAveragePooling2D(),\n",
    "        Dense(1, activation='sigmoid', dtype=tf.float32, kernel_initializer=initializer)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def relu_max(x):\n",
    "    return relu(x, max_value=1)\n",
    "\n",
    "def single_functional_feedforward_model():\n",
    "    l2_reg = 1e-05\n",
    "    gn = 5e-04\n",
    "    initializer = tf.keras.initializers.he_normal(seed=1) #TruncatedNormal(mean=0.0, stddev=0.07)\n",
    "    img_input = Input(shape=single_input_shape)\n",
    "#     x = GaussianNoise(gn)(img_input)\n",
    "#     x = Reshape((single_input_shape))(x)\n",
    "    lay1,lay2,lay3,dense = 8,16,32,8 #32,64,128,32  16,32,64,16\n",
    "    x = Conv2D(lay1, kernel_size=(3,3), input_shape=single_input_shape, strides=(1, 1), data_format = 'channels_last',\\\n",
    "               kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(img_input)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    x = Conv2D(lay2, kernel_size=(3,3), strides=(1, 1), kernel_regularizer=l2(l2_reg),\\\n",
    "               kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    x = Conv2D(lay3, kernel_size=(3,3), strides=(1, 1), kernel_regularizer=l2(l2_reg),\\\n",
    "               kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    x = Dense(dense, kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Dense(1, activation=relu_max, dtype=tensorflow.float32)(x)\n",
    "#     x = Flatten()(x)\n",
    "    out_metric = Dense(1, activation='sigmoid', dtype=tf.float32, name=metric, kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs=img_input, outputs=out_metric)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def joe_test_feedforward_model(): # training pearson = 0.75 vaidation = 0.11\n",
    "    l2_reg = 0 #0.0005\n",
    "    gn = 0.005\n",
    "    initializer = tf.keras.initializers.he_normal(seed=1)\n",
    "    #initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.07)\n",
    "    img_input = Input(shape=single_input_shape)\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    lay1,lay2,lay3,dense = 16,32,64,16 #16,32,64,16\n",
    "    x = Conv2D(lay1, kernel_size=(3,3), input_shape=single_input_shape, strides=(2,2), data_format = 'channels_last',\\\n",
    "               kernel_regularizer=l2(l2_reg), padding = 'same', kernel_initializer=initializer)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    x = ReLU()(x)\n",
    "    x_skip = x\n",
    "    x = Conv2D(lay1, kernel_size=(3,3), input_shape=single_input_shape, strides=2, data_format = 'channels_last',\\\n",
    "               kernel_regularizer=l2(l2_reg), padding = 'same', kernel_initializer=initializer)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_skip]) \n",
    "    print(x.shape)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    x_skip = x\n",
    "    x = Conv2D(lay2, kernel_size=(3,3), strides=2, kernel_regularizer=l2(l2_reg), padding = 'same', kernel_initializer=initializer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(lay2, kernel_size=(3,3), strides=(1, 1), kernel_regularizer=l2(l2_reg),  padding = 'same', kernel_initializer=initializer)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_skip = Conv2D(32, (1,1), strides = (2,2), kernel_initializer=initializer)(x_skip)\n",
    "    # Add Residue\n",
    "    x = Add()([x, x_skip])  \n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Conv2D(lay3, kernel_size=(3,3), strides=(1, 1), kernel_regularizer=l2(l2_reg), kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(dense, kernel_initializer=initializer)(x)\n",
    "    x = ReLU()(x)\n",
    "#     x = ELU(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Dense(1, activation=relu_max, dtype=tensorflow.float32)(x)\n",
    "#     x = Flatten()(x)\n",
    "    out_metric = Dense(1, activation='sigmoid', dtype=tf.float32, name=metric, kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs=img_input, outputs=out_metric)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6e209",
   "metadata": {},
   "source": [
    "### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ce4ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:52.608120Z",
     "start_time": "2022-05-25T22:14:52.177284Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUTOENCODER MODEL\n",
    "def build_autoencoder(input_shape):\n",
    "    l2_reg = 0\n",
    "    gn = 0 #0.0005\n",
    "#     initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.07) #he_normal(seed=1)\n",
    "#     img_input = Input(shape=single_input_shape)\n",
    "#     GaussianNoise(gn)(img_input)\n",
    "#     Reshape((single_input_shape))\n",
    "    lay1,lay2,lay3,dense = 16,8,8 ,64 #4,8,16,8 #16,32,64,32 # 8,16,32,16\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(lay1, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(lay2, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(lay3, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoder = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # at this point the representation is (32, 32, 8)\n",
    "\n",
    "    x = Conv2D(lay3, (3, 3), activation='relu', padding='same')(encoder)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(lay2, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(lay1, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoder = Conv2D(1, (3, 3), activation=relu_max, padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoder)\n",
    "\n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427d777",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee237605",
   "metadata": {},
   "source": [
    "### Introduction and Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3031f",
   "metadata": {},
   "source": [
    "The `compile` method configures the learning process. This involves specifying:\n",
    "* An optimizer https://keras.io/optimizers/. This is what controls how the model updates based on the value of the loss function.\n",
    "* A loss fucntion https://keras.io/losses/\n",
    "* A list of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a97c55",
   "metadata": {},
   "source": [
    "**Defining the shape values as variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d0e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:54.559120Z",
     "start_time": "2022-05-25T22:14:54.191883Z"
    }
   },
   "outputs": [],
   "source": [
    "x_im, y_im, dvf_contour_im = 256,256,2 # 256,256,4 # 384,384,3   # 512,512,3 # 256,256,3\n",
    "single_input_shape = (x_im, y_im, dvf_contour_im)\n",
    "flat_single_input_shape = (x_im*y_im*dvf_contour_im)\n",
    "\n",
    "image_input = (256, 256, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884ec35",
   "metadata": {},
   "source": [
    "**Assign model as a variable and compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dce2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:24:33.301381Z",
     "start_time": "2022-05-25T22:24:32.340409Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras import metrics\n",
    "#from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "# Grab the model from our avaliable set of models\n",
    "\n",
    "# model = single_functional_feedforward_model()\n",
    "\n",
    "model = joe_test_feedforward_model()\n",
    "# model = vector_model()\n",
    "\n",
    "# model = vector_model_sequential()\n",
    "\n",
    "# model = vector_avg_pooling_model()\n",
    "\n",
    "# model = vector_feedforward_model()\n",
    "\n",
    "#Compile model - we tried two different optimizers, one with momentum (adam) and one without\n",
    "#Adadelta = tensorflow.optimizers.Adadelta() #Default values: learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False\n",
    "\n",
    "Adam = tf.optimizers.Adam(learning_rate=1e-03)\n",
    "RMSprop = tf.optimizers.RMSprop(lr=1e-03) #SGD #lr=float(learning_rate), momentum=0.9, nesterov=True\n",
    "# Adadelta = tf.optimizers.Adadelta(lr=1.5, rho=0.95, epsilon=1e-08, decay=0.001)\n",
    "optimizer_choice_string = \"Adam\" # \"Adam\" \"RMSprop\" \"Adadelta\"\n",
    "optimizer_choice = Adam #RMSprop #Adam #Adadelta\n",
    "loss_fn = \"mean_squared_error\" # sparse_categorical_crossentropy mean_squared_error\n",
    "model.compile(loss=loss_fn, optimizer=optimizer_choice)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e4bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:57.187274Z",
     "start_time": "2022-05-25T22:14:56.952782Z"
    }
   },
   "outputs": [],
   "source": [
    "# If using an autoencoder for the image input. Not currently used as a part of the model but will be left here in case of future interest\n",
    "autoencoder = build_autoencoder(image_input)[2]\n",
    "ae_loss_fn = 'mean_squared_error'\n",
    "autoencoder.compile(loss=ae_loss_fn, optimizer=optimizer_choice)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e81ce3",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541678c8",
   "metadata": {},
   "source": [
    "### Preparing the Training and Validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ecfdd",
   "metadata": {},
   "source": [
    "The `fit` function is used for training. An epoch is 'one complete presentation of the data set to be learned to a learning machine'. Many epochs are needed to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9327366",
   "metadata": {},
   "source": [
    "If loading in the data use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c61f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:57.450065Z",
     "start_time": "2022-05-25T22:14:57.190102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalise any array (e.g. pixel brightness) for chosen upper and lower bounds and indexes\n",
    "def normalise_multiple(array, lower_bound, upper_bound, lower_index, upper_index):\n",
    "            \n",
    "    if lower_index==0:\n",
    "        component='pixel brightness'\n",
    "    if lower_index==1 and upper_index==2:\n",
    "        component='DVF x'    \n",
    "    if lower_index==1 and upper_index==3:\n",
    "        component='DVF x & y'\n",
    "    if lower_index==2 and upper_index==3:\n",
    "        component='DVF y'\n",
    "    \n",
    "    print(\"Normalising {} component of slices...\".format(component))\n",
    "    \n",
    "    norm_array = array[:,:,:,lower_index:upper_index] if array.ndim==4 else array\n",
    "\n",
    "    max_value = np.max(norm_array); min_value = np.min(norm_array)\n",
    "    norm_array = ((upper_bound-lower_bound)*( (norm_array-min_value) / (max_value-min_value) )) + lower_bound\n",
    "\n",
    "    print(\"Min value: \", np.min(norm_array), \"\\t Max value: \", np.max(norm_array))\n",
    "\n",
    "    return norm_array\n",
    "\n",
    "# Normalise two separate arrays with respect to their joint array\n",
    "def normalise_train_and_validate(array1, array2, lower_bound, upper_bound, lower_index, upper_index):\n",
    "#     This is the formatting of the data i.e. first dimension is pixel brightness, then DVF_x,y...\n",
    "    if lower_index==0 and upper_index==1:\n",
    "        component='pixel brightness'\n",
    "    if lower_index==1 and upper_index==2:\n",
    "        component='DVF x'    \n",
    "    if lower_index==1 and upper_index==3:\n",
    "        component='DVF x & y'\n",
    "    if lower_index==2 and upper_index==3:\n",
    "        component='DVF y'\n",
    "    if lower_index==3:\n",
    "        component='reference contour'\n",
    "        \n",
    "    print(\"Normalising {} component of slices...\".format(component))\n",
    "    \n",
    "    if array1.ndim==4 and array2.ndim==4:\n",
    "        norm_array1 = array1[:,:,:,lower_index:upper_index]\n",
    "        norm_array2 = array2[:,:,:,lower_index:upper_index]\n",
    "        \n",
    "        min_value = np.min(norm_array1) if np.min(norm_array1) <= np.min(norm_array2) else np.min(norm_array2)\n",
    "        max_value = np.max(norm_array1) if np.max(norm_array1) >= np.max(norm_array2) else np.max(norm_array2)\n",
    "        \n",
    "#         print(\"Min value: \", min(np.min(norm_array1), np.min(norm_array2)),\\\n",
    "#       \"\\t Max value: \", max(np.max(norm_array1), np.max(norm_array2)))\n",
    "        \n",
    "    else:\n",
    "        norm_array1 = array1\n",
    "        norm_array2 = array2\n",
    "#         min and max of the joint array\n",
    "        min_value = np.min(norm_array1) if np.min(norm_array1) <= np.min(norm_array2) else np.min(norm_array2)\n",
    "        max_value = np.max(norm_array1) if np.max(norm_array1) >= np.max(norm_array2) else np.max(norm_array2)\n",
    "        \n",
    "#         print(\"Min value: \", min(np.min(norm_array1), np.min(norm_array2)),\\\n",
    "#       \"\\t Max value: \", max(np.max(norm_array1), np.max(norm_array2)))\n",
    "    # this is the equation for normalising any array\n",
    "    norm_array1 = ((upper_bound-lower_bound)*( (norm_array1-min_value) / (max_value-min_value) )) + lower_bound\n",
    "    norm_array2 = ((upper_bound-lower_bound)*( (norm_array2-min_value) / (max_value-min_value) )) + lower_bound\n",
    "\n",
    "    print(\"Min value: \", min(np.min(norm_array1), np.min(norm_array2)),\\\n",
    "          \"\\t Max value: \", max(np.max(norm_array1), np.max(norm_array2)))\n",
    "    \n",
    "    if array1.ndim==4 and array2.ndim==4:\n",
    "        if upper_index-lower_index==1:\n",
    "            norm_array1 = np.reshape(norm_array1, (len(norm_array1), x_im, y_im))\n",
    "            norm_array2 = np.reshape(norm_array2, (len(norm_array2), x_im, y_im)) \n",
    "        else:\n",
    "            norm_array1 = np.reshape(norm_array1, (len(norm_array1), x_im, y_im, upper_index-lower_index))\n",
    "            norm_array2 = np.reshape(norm_array2, (len(norm_array2), x_im, y_im, upper_index-lower_index))\n",
    "    # arrays must be reshaped otherwise an error will occur\n",
    "    return norm_array1, norm_array2\n",
    "\n",
    "# Identify slices where the DVFs are constant i.e. invalid for use, or remove DVFs above a certain value\n",
    "def remove_invalid_DVF_slices(array, lower_index, upper_index, min_dvf, max_dvf, dvf_cut_off=None):\n",
    "    print('Removing slices containing invalid DVFs...')\n",
    "    invalid_slice, cut_off = [], []\n",
    "    for i in range(len(array)):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        if upper_index-lower_index==2: #and lower_index!=0:\n",
    "            DVF_array1 = array[i,:,:,lower_index]\n",
    "            DVF_array2 = array[i,:,:,upper_index-1]\n",
    "            \n",
    "            if dvf_cut_off==True: # bool choice to remove DVF above or below certain value\n",
    "                if max_dvf<np.max(DVF_array1) or max_dvf<np.max(DVF_array2):\n",
    "                    invalid_slice.append(i)\n",
    "                if min_dvf>np.min(DVF_array1) or min_dvf>np.min(DVF_array2):\n",
    "                    invalid_slice.append(i)\n",
    "                    \n",
    "            elif np.min(DVF_array1)==np.max(DVF_array1) or np.min(DVF_array2)==np.max(DVF_array2):\n",
    "#                 print(np.min(DVF_array1), np.max(DVF_array1), np.min(DVF_array2), np.max(DVF_array2))\n",
    "                invalid_slice.append(i)\n",
    "            elif np.min(DVF_array1)==np.max(DVF_array1) and np.min(DVF_array2)==np.max(DVF_array2):\n",
    "                invalid_slice.append(i)\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "    invalid_slices = np.array(invalid_slice)\n",
    "#     cut_off_slices = np.array(cut_off)\n",
    "#     valid_cut_off_slices = np.argwhere(np.isin(np.arange(len(array)), cut_off_slices, invert=True)).ravel()\n",
    "    valid_slices = np.argwhere(np.isin(np.arange(len(array)), invalid_slices, invert=True)).ravel()\n",
    "    \n",
    "    return array[valid_slices], valid_slices, invalid_slices  #, array[valid_cut_off_slices], cut_off_slices\n",
    "\n",
    "# Remove DVF values where the pixel brightness is zero - didn't improve results when we tried but may still be useful\n",
    "def remove_zero_pixel_DVFs(array, lower_index, upper_index):\n",
    "    print('Removing DVF values where the prixel brightness is 0 ...')\n",
    "    zero_pixel_indexes = np.argwhere(array[:,:,:,0]==0)\n",
    "    for i in range(len(zero_pixel_indexes)):\n",
    "#         if zero_pixel_indexes[i,0]%250==0 and zero_pixel_indexes[i,1]%250==0 and zero_pixel_indexes[i,2]%250==0:\n",
    "#             print(zero_pixel_indexes[i,0])\n",
    "        array[zero_pixel_indexes[i,0], zero_pixel_indexes[i,1], zero_pixel_indexes[i,2], lower_index:upper_index] = 0\n",
    "        \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b28532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:57.904016Z",
     "start_time": "2022-05-25T22:14:57.454245Z"
    }
   },
   "outputs": [],
   "source": [
    "# if organ=='BRAIN_STEM':\n",
    "#     n_train_max = \n",
    "#     n_validate_max = \n",
    "# Choose to include flipped images or not\n",
    "flipped = False\n",
    "\n",
    "if organ=='PAROTID_LT':\n",
    "    if flipped==True:\n",
    "        n_train_max = 28706\n",
    "        n_validate_max = 18652\n",
    "    else:\n",
    "        n_train_max = 14353\n",
    "        n_validate_max = 9326\n",
    "    \n",
    "# Choose number of training and testing samples to use\n",
    "n_train = n_train_max  #n_train_max \n",
    "n_validate = n_validate_max  #n_validate_max \n",
    "\n",
    "# Choose data type of arrays\n",
    "data_type = 'float16'\n",
    "\n",
    "# Choose either to remove zero dice scores, train CNN or autoencoder\n",
    "remove_zero_dice_scores = False\n",
    "train_CNN = True\n",
    "train_AE = False\n",
    "\n",
    "# Set parameters for normalising either the pixel brightness or DVF components\n",
    "Image_norm_min = 0; Image_norm_max = 1\n",
    "DVF_norm_min = -1; DVF_norm_max = 1\n",
    "\n",
    "# # slices of shape (n, x, y, [dvf_x, dvf_y])\n",
    "\n",
    "# Set dvf removal parameters\n",
    "bool_cut_slices = False; dvf_min_cut_off = -3500; dvf_max_cut_off = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9bdcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:58.382734Z",
     "start_time": "2022-05-25T22:14:57.912471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Folders for accessing and loading the slice data\n",
    "organ = 'PAROTID_LT'\n",
    "slice_array_folder = '/hepgpu9-data1/sbridger/mphys_2d_network_2021-22/'\\\n",
    "'slice_arrays/new_registrations/{}/complete_arrays/'.format(organ)\n",
    "train_folder = slice_array_folder+'train_odd_{}_'.format(organ)\n",
    "test_folder = slice_array_folder+'test_even_{}_'.format(organ)\n",
    "\n",
    "# Set as this if normalising and transforming metric arrays after removing invalid DVFs etc.\n",
    "metric_choice = 'dice'\n",
    "train_metric_array = np.load(train_folder+'multiple_{}_array.npy'.format(metric_choice), mmap_mode='c')[:n_train]\n",
    "test_metric_array = np.load(test_folder+'multiple_{}_array.npy'.format(metric_choice), mmap_mode='c')[:n_validate]\n",
    "Metrics = np.concatenate((train_metric_array, test_metric_array)) \n",
    "\n",
    "if remove_zero_dice_scores and metric_choice=='dice':\n",
    "    print('Removing zero dice scores...')\n",
    "    nz_dice_scores_train = np.where(Metrics[:n_train]>0)[0]\n",
    "    nz_dice_scores_validate = np.where(Metrics[n_train:]>0)[0]\n",
    "    \n",
    "    train_indices = nz_dice_scores_train[:n_train]\n",
    "    validate_indices = nz_dice_scores_validate[:n_validate]\n",
    "    \n",
    "#     X_train_mmap = np.load('PAROTID_LT_multiple_train_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[train_indices]#.astype(data_type)\n",
    "#     X_validate_mmap = np.load('PAROTID_LT_multiple_validate_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[validate_indices]#.astype(data_type)\n",
    "    \n",
    "    X_train_mmap = np.load('PAROTID_LT_multiple_train_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[train_indices]\n",
    "    X_validate_mmap = np.load('PAROTID_LT_multiple_validate_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[validate_indices]\n",
    "    \n",
    "#     X_train_mmap = np.load('PAROTID_LT_multiple_train_ref_image_reg_contour_plus_LR_flipped_{}_normalised_array.npy'.format(x_im), mmap_mode = 'c')[:n_train]\n",
    "#     X_validate_mmap = np.load('PAROTID_LT_multiple_validate_ref_image_reg_contour_plus_LR_flipped_{}_normalised_array.npy'.format(x_im), mmap_mode = 'c')[:n_validate]\n",
    "\n",
    "    print('Initial shapes of training and validation data:')\n",
    "    print(X_train_mmap.shape, X_validate_mmap.shape)\n",
    "    \n",
    "#     Remove slices with invalid DVFs\n",
    "#     X_train_mmap, valid_train_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_train_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "#     X_validate_mmap, valid_validation_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_validate_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "#     print('Shapes of training and validation data after removing slices with invalid DVFs:')\n",
    "#     print(X_train_mmap.shape, X_validate_mmap.shape)\n",
    "    \n",
    "#     Normalise images\n",
    "\n",
    "    X_train_mmap[:,:,:,0], X_validate_mmap[:,:,:,0] =\\\n",
    "    normalise_train_and_validate(X_train_mmap, X_validate_mmap, Image_norm_min, Image_norm_max, 0, 1)\n",
    "    \n",
    "#     Normalise DVF x & y components\n",
    "#     X_train_mmap[:,:,:,1:3], X_validate_mmap[:,:,:,1:3] =\\\n",
    "#     normalise_train_and_validate(X_train_mmap, X_validate_mmap, DVF_norm_min, DVF_norm_max, 1, 3)\n",
    "\n",
    "\n",
    "if train_CNN:\n",
    "    print('Preparing CNN slice data...')\n",
    "    \n",
    "#     X_train_mmap = np.load('PAROTID_LT_multiple_train_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[:n_train]\n",
    "#     X_validate_mmap = np.load('PAROTID_LT_multiple_validate_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[:n_validate]\n",
    "    \n",
    "#     X_train_mmap = np.load('PAROTID_LT_multiple_train_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[:n_train]\n",
    "#     X_validate_mmap = np.load('PAROTID_LT_multiple_validate_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[:n_validate]\n",
    "    \n",
    "#     X_train_mmap = np.load('PAROTID_LT_multiple_train_ref_image_dvf_xy_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[:n_train]\n",
    "#     X_validate_mmap = np.load('PAROTID_LT_multiple_validate_ref_image_dvf_xy_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'c')[:n_validate]    \n",
    "\n",
    "\n",
    "    X_train_mmap = np.load('PAROTID_LT_multiple_train_ref_image_reg_contour_plus_LR_flipped_{}_normalised_array.npy'.format(x_im), mmap_mode = 'c')[:n_train]\n",
    "    X_validate_mmap = np.load('PAROTID_LT_multiple_validate_ref_image_reg_contour_plus_LR_flipped_{}_normalised_array.npy'.format(x_im), mmap_mode = 'c')[:n_validate]\n",
    "    \n",
    "    print('Initial shapes of training and validation data:')\n",
    "    print(X_train_mmap.shape, X_validate_mmap.shape)\n",
    "    \n",
    "#     Remove slices with invalid DVFs\n",
    "#     X_train_mmap, valid_train_indexes, invalid_train_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_train_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "#     X_validate_mmap, valid_validation_indexes, invalid_validation_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_validate_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "#     print('Shapes of training and validation data after removing slices with invalid DVFs:')\n",
    "#     print(X_train_mmap.shape, X_validate_mmap.shape)\n",
    "\n",
    "#     Normalise images & contour images:\n",
    "\n",
    "#     Take log(1+PB) of pixel brightness\n",
    "\n",
    "#     X_train_mmap[:,:,:,0], X_validate_mmap[:,:,:,0] =\\\n",
    "#     np.log(X_train_mmap[:,:,:,0]+1), np.log(X_validate_mmap[:,:,:,0]+1)\n",
    "\n",
    "#     X_train_mmap[:,:,:,0], X_validate_mmap[:,:,:,0] =\\\n",
    "#     normalise_train_and_validate(X_train_mmap, X_validate_mmap, Image_norm_min, Image_norm_max, 0, 1)\n",
    "\n",
    "#     X_train_mmap[:,:,:,3], X_validate_mmap[:,:,:,3] =\\\n",
    "#     normalise_train_and_validate(X_train_mmap, X_validate_mmap, Image_norm_min, Image_norm_max, 3, 4)\n",
    "    \n",
    "#     Remove DVFs where the pixel brightness is 0:\n",
    "#     X_train_mmap = remove_zero_pixel_DVFs(X_train_mmap, 1, 3)\n",
    "#     X_validate_mmap = remove_zero_pixel_DVFs(X_validate_mmap, 1, 3)\n",
    "\n",
    "#     Normalise DVF x & y components    \n",
    "#     X_train_mmap[:,:,:,1:3], X_validate_mmap[:,:,:,1:3] =\\\n",
    "#     normalise_train_and_validate(X_train_mmap, X_validate_mmap, DVF_norm_min, DVF_norm_max, 1, 3)\n",
    "\n",
    "if train_AE:\n",
    "    print('Preparing autoencoder slice data...')\n",
    "    X_train_mmap_load = np.load('PAROTID_LT_multiple_train_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[:n_train,:,:,0]#.astype(data_type)\n",
    "    X_validate_mmap_load = np.load('PAROTID_LT_multiple_validate_patient_image_dvf_xy_256_array.npy', mmap_mode = 'c')[:n_validate,:,:,0]#.astype(data_type)\n",
    "#     print(X_train_mmap.shape, X_validate_mmap.shape)\n",
    "\n",
    "#     Remove slices with invalid DVFs\n",
    "#     X_train_mmap_load, valid_train_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_train_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "#     X_validate_mmap_load, valid_validation_indexes =\\\n",
    "#     remove_invalid_DVF_slices(X_validate_mmap, 1, 3, dvf_min_cut_off, dvf_max_cut_off, bool_cut_slices)\n",
    "    \n",
    "    #     Normalise images\n",
    "    X_train_mmap_norm, X_validate_mmap_norm =\\\n",
    "    normalise_train_and_validate(X_train_mmap_load, X_validate_mmap_load, Image_norm_min, Image_norm_max, 0, 1)\n",
    "    \n",
    "    X_train_mmap = np.reshape(X_train_mmap_norm, (len(X_train_mmap_norm), x_im, y_im, 1))\n",
    "    X_validate_mmap = np.reshape(X_validate_mmap_norm, (len(X_validate_mmap_norm), x_im, y_im, 1))\n",
    "    \n",
    "    print(X_train_mmap.shape, X_validate_mmap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08d6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:58.840510Z",
     "start_time": "2022-05-25T22:14:58.387362Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to include data where the images have been flipped then uncomment this code\n",
    "# X_train_mmap_flipped = np.empty(X_train_mmap.shape)\n",
    "# X_validate_mmap_flipped = np.empty(X_validate_mmap.shape)\n",
    "\n",
    "# for i in range(len(X_train_mmap)):\n",
    "#     for j in range(2):\n",
    "#         X_train_mmap_flipped[i,:,:,j] = np.fliplr(X_train_mmap[i,:,:,j])\n",
    "    \n",
    "# for i in range(len(X_validate_mmap)): \n",
    "#     for j in range(2):\n",
    "#         X_validate_mmap_flipped[i,:,:,j] = np.fliplr(X_validate_mmap[i,:,:,j])\n",
    "\n",
    "# Just a test to see the effect of the tranformed images\n",
    "# for i in range(1):\n",
    "#     for j in range(2):\n",
    "#         print(i,j)\n",
    "#         plt.imshow(X_train_mmap[i,:,:,j].astype(np.float32))\n",
    "#         plt.show()\n",
    "#         plt.imshow(X_train_mmap_flipped[i,:,:,j].astype(np.float32))\n",
    "#         plt.show()\n",
    "#         plt.imshow(X_validate_mmap[i,:,:,j].astype(np.float32))\n",
    "#         plt.show()\n",
    "#         plt.imshow(X_validate_mmap_flipped[i,:,:,j].astype(np.float32))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446b7e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:14:59.357622Z",
     "start_time": "2022-05-25T22:14:58.844395Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#Pixel brightness/ DVF histogram for visualisation of the data\n",
    "# pixel_array = np.concatenate(( X_train_mmap[:,:,:,0], X_validate_mmap[:,:,:,0] )).astype(np.float32)\n",
    "# array = pixel_array.ravel()\n",
    "# n_bins = 50\n",
    "# fig, ax1 = plt.subplots(figsize=(5, 3.5))\n",
    "# #removes the grey background to the plot\n",
    "# fig.patch.set_facecolor('white')\n",
    "# plt.hist(array, weights=None, density = False, bins = n_bins)\n",
    "# plt.title('Log pixel brightness distribution', y=1.12)\n",
    "# plt.xlabel('Log pixel brightness')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# # plt.title('DVF y-component distribution', y=1.12)\n",
    "# # plt.xlabel('DVF y-component value')\n",
    "# plt.ylim(0,3.5e8)\n",
    "# # plt.xlim(0,1)\n",
    "# plt.tick_params(axis='both', which='major')\n",
    "# plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0), useMathText=True)\n",
    "# plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0), useMathText=True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(1000):#len(pixel_array)):\n",
    "# #     pixel_array = X_train_mmap_test[i,:,:,0]\n",
    "# #     pixel_array32 = np.float32(pixel_array)\n",
    "#     #print(pixel_array32.dtype)\n",
    "#     im_array = pixel_array[i,:,:]\n",
    "#     fig = plt.figure(figsize=(5, 3.5))\n",
    "#     fig.patch.set_facecolor('white')\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.set_title('Pixel brightness colour map')\n",
    "#     plt.imshow(im_array)\n",
    "#     ax.set_aspect('equal')\n",
    "\n",
    "#     #cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "#     # cax.get_xaxis().set_visible(False)\n",
    "#     # cax.get_yaxis().set_visible(False)\n",
    "#     # cax.patch.set_alpha(0)\n",
    "#     # cax.set_frame_on(False)\n",
    "#     plt.colorbar(orientation='vertical')\n",
    "# #     plt.clim(0, 4700)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46349b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:15:00.008516Z",
     "start_time": "2022-05-25T22:14:59.360929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving image arrays by joining them up in the manner you see fit - again, adapt this to save time if creating your\n",
    "# own data sets.\n",
    "# x1 = np.reshape(X_train_mmap[:,:,:,0], (len(X_train_mmap),256,256,1)) \n",
    "# x2 = np.reshape(X_train_mmap[:,:,:,3], (len(X_train_mmap),256,256,1))\n",
    "# print(x1.shape, x2.shape)\n",
    "# x3 = np.reshape(X_validate_mmap[:,:,:,0], (len(X_validate_mmap),256,256,1)) \n",
    "# x4 = np.reshape(X_validate_mmap[:,:,:,3], (len(X_validate_mmap),256,256,1))\n",
    "# X_train_mmap = np.concatenate((x1,x2), axis=3) # X_train_mmap\n",
    "# print(X_train_mmap.shape)\n",
    "# X_validate_mmap = np.concatenate((x3,x4), axis=3) # X_validate_mmap\n",
    "# X_train_mmap = X_train_mmap[:,:,:,0]\n",
    "# X_validate_mmap = X_validate_mmap[:,:,:,0]\n",
    "# X_train_mmap = np.reshape(X_train_mmap, (len(X_train_mmap),x_im,x_im,1))\n",
    "# X_validate_mmap = np.reshape(X_validate_mmap, (len(X_validate_mmap),x_im,x_im,1))\n",
    "\n",
    "# X_train_def_contours = np.load(slice_array_folder+'train_odd_PAROTID_LT_multiple_def_contour_image_array.npy', mmap_mode='c')[:,128:-128,128:-128]\n",
    "# X_validate_def_contours = np.load(slice_array_folder+'test_even_PAROTID_LT_multiple_def_contour_image_array.npy', mmap_mode='c')[:,128:-128,128:-128]\n",
    "# print('here')\n",
    "# X_train_def_contours = np.reshape(X_train_def_contours, (len(X_train_def_contours),x_im,x_im,1))\n",
    "# X_validate_def_contours = np.reshape(X_validate_def_contours, (len(X_validate_def_contours),x_im,x_im,1))\n",
    "# print(X_train_def_contours.shape, X_train_mmap.shape)\n",
    "# X_train_joined = np.concatenate((X_train_mmap, X_train_def_contours), axis=3)\n",
    "# X_validate_joined = np.concatenate((X_validate_mmap, X_validate_def_contours), axis=3)\n",
    "\n",
    "# np.save('PAROTID_LT_multiple_train_image_ref_reg_contour_{}_array.npy'.format(x_im), X_train_joined)\n",
    "# np.save('PAROTID_LT_multiple_validate_image_ref_reg_contour_{}_array.npy'.format(x_im), X_validate_joined)\n",
    "\n",
    "# np.save('PAROTID_LT_multiple_train_ref_reg_contour_image_dvf_array_{}_array.npy'.format(x_im), X_train_joined)\n",
    "# np.save('PAROTID_LT_multiple_validate_ref_reg_contour_image_dvf_array_{}_array.npy'.format(x_im), X_validate_joined)\n",
    "\n",
    "# np.save('PAROTID_LT_multiple_train_ref_image_dvf_reg_contour_{}_array.npy'.format(x_im), X_train_joined)\n",
    "# np.save('PAROTID_LT_multiple_validate_ref_image_dvf_reg_contour_{}_array.npy'.format(x_im), X_validate_joined)\n",
    "# print(X_train_joined.shape)\n",
    "\n",
    "# For downsizing the images change the value of cut:\n",
    "#     cut = 128\n",
    "#     X_train_mmap = np.load(train_folder+'multiple_ref_contour_image_array.npy', mmap_mode = 'c')[:,cut:-cut,cut:-cut]\n",
    "#     X_validate_mmap = np.load(test_folder+'multiple_ref_contour_image_array.npy', mmap_mode = 'c')[:,cut:-cut,cut:-cut]\n",
    "#     np.save('PAROTID_LT_multiple_train_ref_contour_image_array_256_array.npy', X_train_mmap)\n",
    "#     np.save('PAROTID_LT_multiple_validate_ref_contour_image_array_256_array.npy', X_validate_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da35c16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:15:00.877243Z",
     "start_time": "2022-05-25T22:15:00.012559Z"
    }
   },
   "outputs": [],
   "source": [
    "# #     Metrics & Weights\n",
    "metric_choice = 'dice'\n",
    "train_metric_array = np.load(train_folder+'multiple_{}_array.npy'.format(metric_choice), mmap_mode='c')[:n_train]\n",
    "test_metric_array = np.load(test_folder+'multiple_{}_array.npy'.format(metric_choice), mmap_mode='c')[:n_validate]\n",
    "Metrics = np.concatenate((train_metric_array, test_metric_array)) \n",
    "\n",
    "valid_train_indexes = np.arange(len(train_metric_array))\n",
    "valid_validation_indexes = np.arange(len(train_metric_array),len(train_metric_array)+len(test_metric_array))\n",
    "valid_indexes = np.concatenate((valid_train_indexes, valid_validation_indexes))\n",
    "\n",
    "if metric_choice == \"rms\" or metric_choice == \"msd\":\n",
    "    power = 1\n",
    "else:\n",
    "    power = 1\n",
    "\n",
    "Metrics = transformed_array(Metrics[valid_indexes], power, metric_choice)\n",
    "\n",
    "\n",
    "if flipped==True:\n",
    "    Y_train_copy = Metrics[valid_train_indexes].astype(data_type)\n",
    "    Y_validate_copy = Metrics[valid_validation_indexes].astype(data_type)\n",
    "\n",
    "    Y_train_mmap = np.concatenate((Y_train_copy, Y_train_copy))\n",
    "    Y_validate_mmap = np.concatenate((Y_validate_copy, Y_validate_copy))\n",
    "    \n",
    "elif remove_zero_dice_scores and metric_choice=='dice':\n",
    "    print('Zero dice scores metric array chosen')\n",
    "    Y_train_mmap = Metrics[train_indices].astype(data_type)\n",
    "    Y_validate_mmap = Metrics[validate_indices].astype(data_type)\n",
    "    \n",
    "else:\n",
    "    Y_train_mmap = Metrics[valid_train_indexes].astype(data_type)\n",
    "    Y_validate_mmap = Metrics[valid_validation_indexes].astype(data_type)\n",
    "\n",
    "# Weights\n",
    "weight_array = binningWeights(Metrics, n_bins, 3)# weightArray\n",
    "weight_array, mn, mx = normalise(weight_array)\n",
    "W_train_mmap = weight_array[valid_train_indexes].astype(data_type)\n",
    "\n",
    "# W_train_mmap = weight_array[train_indices].astype(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c90da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:15:01.276231Z",
     "start_time": "2022-05-25T22:15:00.879074Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = [\"training \", \"validation \"]\n",
    "# n_bins=60\n",
    "if train_CNN or remove_zero_dice_scores:\n",
    "    for i in range(2):\n",
    "        fig, ax1 = plt.subplots(figsize=(5, 3.5))\n",
    "        Y_array=[Y_train_mmap, Y_validate_mmap]\n",
    "        #removes the grey background to the plot\n",
    "        fig.patch.set_facecolor('white')\n",
    "        plt.hist(Y_array[i], weights=None, density = False, bins = n_bins, range = (0,1))\n",
    "        ax1.set_ylabel('Frequency', fontsize=18)\n",
    "        ax1.set_xlabel(label_list[i] + metric_choice, fontsize=18)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663e34f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fdb5a",
   "metadata": {},
   "source": [
    "This built-in tensorflow function **Trains** the model for a fixed number of epochs(iterations on a dataset)\n",
    "```\n",
    "model.fit(\n",
    "    x=None, y=None, batch_size=None, epochs=1, verbose='auto',\n",
    "    callbacks=None, validation_split=0.0, validation_data=None, shuffle=True,\n",
    "    class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10, workers=1, use_multiprocessing=False\n",
    ")\n",
    "```\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit - details what all the input parameters mean\n",
    "* https://medium.com/analytics-vidhya/train-keras-model-with-large-dataset-batch-training-6b3099fdf366\n",
    "* https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fdde7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:15:01.280197Z",
     "start_time": "2022-05-25T22:15:01.277841Z"
    }
   },
   "outputs": [],
   "source": [
    "# If using autoencoder\n",
    "# batch_size = 32\n",
    "# epoch_choice = 4\n",
    "\n",
    "# x_train = X_train_mmap[:,:,:,0] # X_train_mmap\n",
    "# x_validate = X_validate_mmap[:,:,:,0] # X_validate_mmap\n",
    "# x_train = np.reshape(x_train, (len(x_train), x_im, y_im, 1))\n",
    "# x_validate = np.reshape(x_validate, (len(x_validate), x_im, y_im, 1))\n",
    "# # Train autoencoder\n",
    "\n",
    "# history = autoencoder.fit(x=x_train, y=x_train, batch_size=batch_size, shuffle=True,\n",
    "#                     validation_data = (x_validate, x_validate),\n",
    "#           epochs=epoch_choice, verbose=1)#, callbacks=callback_list) #[cp_callback])\n",
    "\n",
    "# # Construct the images\n",
    "# # X_train = X_train_mmap; X_validate = X_validate_mmap\n",
    "# X_train = x_train\n",
    "# X_validate = x_validate\n",
    "\n",
    "# AE_predicted_metric_training = autoencoder.predict(X_train, batch_size=batch_size, verbose=0)\n",
    "# AE_predicted_metric_validation = autoencoder.predict(X_validate, batch_size = batch_size, verbose=0)\n",
    "\n",
    "# AE_predicted_training = np.reshape(AE_predicted_metric_training, (len(X_train),256,256))\n",
    "# AE_predicted_validation = np.reshape(AE_predicted_metric_validation, (len(X_validate),256,256))\n",
    "\n",
    "# # View the reconstructed validation images\n",
    "# n_view = 3\n",
    "# for i in range(n_view):\n",
    "#     f, axarr = plt.subplots(1,2)\n",
    "# #     axarr[0].imshow(X_validate[i,:,:,0])\n",
    "#     axarr[1].imshow(AE_predicted_validation[i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d833126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:15:01.787375Z",
     "start_time": "2022-05-25T22:15:01.281664Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "learning_rate = '0.005'\n",
    "momentum = 0.9\n",
    "patience = 20\n",
    "\n",
    "output_folder = '/hepgpu9-data1/sbridger/mphys_2d_network_2021-22/plots/' + organ + '/'\n",
    "\n",
    "ES = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=int(patience), verbose=1, mode='min', baseline=None, \\\n",
    "                            restore_best_weights=True)\n",
    "\n",
    "class LearningRateReducer(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        old_lr = self.model.optimizer.lr.read_value()\n",
    "        new_lr = old_lr * 0.99\n",
    "        print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "        self.model.optimizer.lr.assign(new_lr)\n",
    "\n",
    "reduce_lr_callbacks = [LearningRateReducer()]\n",
    "#callback_list = [ES, TB]\n",
    "#rlrop = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "\n",
    "callback_list = [ES]\n",
    "\n",
    "# Checkpoint callback, period = x means save the model weights after every x epochs.\n",
    "# {epoch:04d} refers to the string of the epoch number, i.e. naming the files by epoch number\n",
    "checkpoint_path='/hepgpu9-data1/sbridger/mphys_2d_network_2021-22/weight_checkpoints/final_model/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_name = \n",
    "cp_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    save_freq=1, \n",
    "    save_weights_only=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2929b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:35:11.008084Z",
     "start_time": "2022-05-25T22:34:28.232531Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = X_train_mmap\n",
    "x_validate = X_validate_mmap\n",
    "y_train = Y_train_mmap # Y_train_mmap\n",
    "y_validate = Y_validate_mmap # Y_validate_mmap\n",
    "# w_train = W_train_mmap\n",
    "# w_train /= np.mean(w_train)\n",
    "# print(np.mean(w_train), np.std(w_train))\n",
    "# w_train = binningWeights(Metrics, n_bins, 0)\n",
    "\n",
    "# Allow for autoencoded output as CNN input\n",
    "# x_train[:,:,:,0] = AE_predicted_training\n",
    "# x_validate[:,:,:,0] = AE_predicted_validation\n",
    "\n",
    "# SPECIFYING IMPORTANT FUNCTION PARAMETERS\n",
    "batch_size = 32\n",
    "epoch_choice = 80\n",
    "\n",
    "# Train CNN\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=batch_size,\n",
    "                    validation_data = (x_validate, y_validate),# sample_weight = w_train,\n",
    "                    shuffle=True, epochs=epoch_choice, verbose=1, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4af9bb",
   "metadata": {},
   "source": [
    "`ImageDataGenerator` is a Python class with many objects that allow you to manipulate the images, such as horizontal flip, vertical flip, rotation. It converts images into tensor image data. Using the above objects you can apply transformations to your training set, which may allow the model to generalise better.\n",
    "* If our data is in a directory we use `flow_from_directory`, this is an attribute of our `ImageDataGenerator` class. \n",
    "* If our data is in a dataframe we use `flow_from_dataframe`, this is an attribute of our `ImageDataGenerator` class.\n",
    "* This class can also rescale all pixel values between 0 and 1 `train_datagen = ImageDataGenerator(rescale=1./255)`, for training, `valid_datagen = ImageDataGenerator(rescale=1./255)`, for validation.\n",
    "* We use the `datagen.flow` built-in function when the data is stored as a variable accessible in the code. \n",
    "\n",
    "```\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "start0 = time.time()\n",
    "train_generator = datagen.flow(X_train, [Y_train1, Y_train2], batch_size=batch_size, shuffle=True,\n",
    "                               sample_weight={metric1: W_train1, metric2: W_train2}, seed=1)\n",
    "\n",
    "validation_generator = datagen.flow(X_validate, [Y_validate1, Y_validate2], \n",
    "                                    batch_size=batch_size, shuffle=True, seed=1,\n",
    "                                    sample_weight={metric1: W_validate1, metric2: W_validate2})\n",
    "stop0 = time.time()\n",
    "print(\"It took \", str(stop0-start0), \" seconds to load in the data.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f509bf",
   "metadata": {},
   "source": [
    "**Callbacks**\n",
    "\n",
    "`tensorflow.keras.callbacks` has many attributes two of which are: \n",
    "\n",
    "* `LearningRateScheduler`.\n",
    "```\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
    "```\n",
    "This allows us to update the learning rate after each epoch. It is useful because it allows us to plot the loss over all learning rate values\n",
    "\n",
    "* `TensorBoard` - as written by Sol\n",
    "\n",
    "```\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "```\n",
    "Enables visualisation concurrently with the machine learning workflow with tensorboard. In describing what `histogram_freq` the document can be quoted as \"frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.\"\n",
    "\n",
    "* https://www.tensorflow.org/tensorboard/get_started\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193e077",
   "metadata": {},
   "source": [
    "**Sol's use of ImageGen and tensorboard**\n",
    "```\n",
    "history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=[tensorboard_callback])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346c234",
   "metadata": {},
   "source": [
    "```\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {logs_base_dir} --host localhost\n",
    "from tensorboard import notebook\n",
    "#notebook.list()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d42b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b4063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.539282Z",
     "start_time": "2022-05-15T11:12:25.289Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, folder, file_name):\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        os.system('mkdir -p {}'.format(folder))\n",
    "        with open(folder+file_name+'.json', \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        print(\"Saved model structure to: %s%s.json\"%(folder,file_name))\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(folder+file_name+\".h5\")\n",
    "        print(\"Saved model weights to: %s%s.h5\"%(folder, file_name))\n",
    "\n",
    "import datetime #module to provide the current date and time for use in the file names\n",
    "currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "modelname = 'model_' + currentDateTime + '_' + optimizer_choice_string + '_' + organ\n",
    "\n",
    "save_model(model, output_folder, modelname)\n",
    "\n",
    "# #save model to an eps file\n",
    "# from keras.utils import plot_model\n",
    "# eps_file = output_folder + modelname + '.eps'\n",
    "# plot_model(model,to_file=eps_file,show_shapes=True, show_layer_names=False)\n",
    "# print(\"Model saved to eps: \" + eps_file)\n",
    "\n",
    "# #display model\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# SVG(model_to_dot(model, show_shapes=True, show_layer_names=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14939c0f",
   "metadata": {},
   "source": [
    "### Plot the loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c11c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T21:55:54.104663Z",
     "start_time": "2022-05-25T21:55:53.800306Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(history,output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # summarize history for loss\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    #plt.xlim(left=10)\n",
    "    loss_values_cropped = loss_values#[10:]\n",
    "    val_loss_values_cropped = val_loss_values#[10:]\n",
    "    loss_dif = np.abs(loss_values_cropped - val_loss_values_cropped)\n",
    "    l1 = ax1.plot(loss_values_cropped,color='cornflowerblue', label='Training loss')\n",
    "    l2 = ax1.plot(val_loss_values_cropped,color='green', label='Validation loss')\n",
    "    l_diff = ax1.plot(loss_dif, color='red', label='Absolute loss difference')\n",
    "    ax1.set_ylabel('Loss', fontsize=18)\n",
    "    ax1.set_xlabel('Epoch', fontsize=18)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    #plt.ylim(0,0.04)\n",
    "    #plt.xlim(0,410)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#     plt.savefig(output_folder + 'acc_loss' + currentDateTime + '_' + optimizer_choice_string + '_' + organ + '.png')\n",
    "#     np.save(output_folder+'validation_loss_history.npy', np.array(val_loss_values))\n",
    "#     np.save(output_folder+'training_loss_history.npy', np.array(loss_values))\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc3c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.543513Z",
     "start_time": "2022-05-15T11:12:25.294Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(history,output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # summarize history for loss\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    #plt.xlim(left=10)\n",
    "    loss_values_cropped = loss_values#[10:]\n",
    "    val_loss_values_cropped = val_loss_values#[10:]\n",
    "    l1 = ax1.plot(loss_values_cropped,color='cornflowerblue', label='Training loss')\n",
    "    l2 = ax1.plot(val_loss_values_cropped,color='green', label='Validation loss')\n",
    "    ax1.set_ylabel('Loss', fontsize=18)\n",
    "    ax1.set_xlabel('Epoch', fontsize=18)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    plt.ylim(0,0.08)\n",
    "    #plt.xlim(0,410)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#     plt.savefig(output_folder + 'acc_loss' + currentDateTime + '_' + optimizer_choice_string + '_' + organ + '_shorter_range_1.png')\n",
    "#     np.save(output_folder+'validation_loss_history.npy', np.array(val_loss_values))\n",
    "#     np.save(output_folder+'training_loss_history.npy', np.array(loss_values))\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history, output_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6bcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.545311Z",
     "start_time": "2022-05-15T11:12:25.296Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(history,output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # summarize history for loss\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    #plt.xlim(left=10)\n",
    "    loss_values_cropped = loss_values#[10:]\n",
    "    val_loss_values_cropped = val_loss_values#[10:]\n",
    "    l1 = ax1.plot(loss_values_cropped,color='cornflowerblue', label='Training loss')\n",
    "    l2 = ax1.plot(val_loss_values_cropped,color='green', label='Validation loss')\n",
    "    ax1.set_ylabel('Loss', fontsize=18)\n",
    "    ax1.set_xlabel('Epoch', fontsize=18)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    plt.ylim(0,0.04)\n",
    "    #plt.xlim(0,410)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#     plt.savefig(output_folder + 'acc_loss' + currentDateTime + '_' + optimizer_choice_string + '_' + organ + '_shorter_range_2.png')\n",
    "#     np.save(output_folder+'validation_loss_history.npy', np.array(val_loss_values))\n",
    "#     np.save(output_folder+'training_loss_history.npy', np.array(loss_values))\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(history, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e85fa6",
   "metadata": {},
   "source": [
    "#### Load a Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0480d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.547000Z",
     "start_time": "2022-05-15T11:12:25.300Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \n",
    "os.system('mkdir -p {}'.format(output_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d73345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.548799Z",
     "start_time": "2022-05-15T11:12:25.302Z"
    }
   },
   "outputs": [],
   "source": [
    "#Specify the json model file name\n",
    "#json_filename = \"/hepgpu9-data1/sbridger/mphys-2d-network-2019-20/plots/BRAIN_STEM/with_zero_dice_scores/MSD_3D/fourth_root/model_20-27_August-16-2021_RMSprop_BRAIN_STEM.json\"\n",
    "json_filename = \n",
    "\n",
    "#Specify the weight h5 filename\n",
    "weights_filename = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46a501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.550534Z",
     "start_time": "2022-05-15T11:12:25.305Z"
    }
   },
   "outputs": [],
   "source": [
    "#open the json file containing the model\n",
    "json_file = open(json_filename, 'r')\n",
    "json_string = json_file.read() #read the json file to a string\n",
    "json_file.close()\n",
    "print('json file read and closed')\n",
    "\n",
    "\n",
    "#load the model from the json string (doesn't include the weights)\n",
    "loaded_model = tensorflow.keras.models.model_from_json(json_string)\n",
    "print('model loaded')\n",
    "#initialise the saved weights\n",
    "loaded_model.load_weights(weights_filename, by_name=False)\n",
    "print('weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf18a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.552362Z",
     "start_time": "2022-05-15T11:12:25.309Z"
    }
   },
   "outputs": [],
   "source": [
    "#### If using a model with custom activation (such as absolute) loading the .json can give problems. Simply define the model architecture and then load the weights (.h5 file) onto it.\n",
    "model.load_weights(weights_filename, by_name=False)\n",
    "print('weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3b028",
   "metadata": {},
   "source": [
    "After running the cells above, the loaded model is stored in the variable \"loaded_model\". This is done as a way of preventing accidental load of a model as \"model\", which would overwrite the model one might have trained. If you want to work with a loaded model, please uncomment and run the following cell (I advice you comment it back afterwards):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a31d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.554204Z",
     "start_time": "2022-05-15T11:12:25.312Z"
    }
   },
   "outputs": [],
   "source": [
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6833d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.556049Z",
     "start_time": "2022-05-15T11:12:25.315Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \"/hepgpu9-data1/plots/\"+organ+\"/after_resizing/single_slices/cartesian/ReLU/\"+metric+\"/fourth_root/batch_\" + str(batch_size)\n",
    "#Load predictions\n",
    "os.system('mkdir -p {}'.format(output_folder))\n",
    "predicted_metric_training = np.load(output_folder + \"/predicted_metric_training_\"+metric+\"\"+organ+\"\"+str(percentage_of_split)+\".npy\")\n",
    "predicted_metric_validation = np.load(output_folder + \"/predicted_metric_validation_\"+metric+\"\"+organ+\"\"+str(percentage_of_split)+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88fd67",
   "metadata": {},
   "source": [
    "### Predict with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2900c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.557705Z",
     "start_time": "2022-05-15T11:12:25.318Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cfba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.559568Z",
     "start_time": "2022-05-15T11:12:25.321Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_metric_training_name = output_folder + '/predicted_metric_training_' + metric+'_'+organ+'_'+\\\n",
    "                                str(percentage_of_split)\n",
    "np.save(predicted_metric_training_name, predicted_metric_training)\n",
    "print(\"Training predictions array saved\")\n",
    "# Must have 3D_MSD as previously metric2=3D_MSD, now changed to MSD_3D\n",
    "predicted_metric_validation_name = output_folder + '/predicted_metric_validation_' + metric+'_'+organ+'_'+\\\n",
    "                                str(percentage_of_split)\n",
    "np.save(predicted_metric_validation_name, predicted_metric_validation)\n",
    "print(\"Validation predictions array saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67047dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.561406Z",
     "start_time": "2022-05-15T11:12:25.325Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_load = np.load('PAROTID_LT_multiple_train_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'r')[:n_train]\n",
    "X_validate_load = np.load('PAROTID_LT_multiple_validate_ref_image_reg_contour_{}_array.npy'.format(x_im), mmap_mode = 'r')[:n_validate]\n",
    "bright=[]\n",
    "arr_choice = X_train_load\n",
    "for i in range(len(arr_choice)):\n",
    "    if np.max(arr_choice[i,:,:,0])>3500:\n",
    "        bright.append(i)\n",
    "print(len(bright))\n",
    "X_bright_val = x_validate[bright]\n",
    "predicted_metric_validation_bright = model.predict(X_bright_val, batch_size = batch_size, verbose=0)\n",
    "predicted_validation_bright = np.reshape(predicted_metric_validation_bright, len(X_bright_val))\n",
    "\n",
    "Y_bright_val = y_validate[bright]\n",
    "pearson_validation_bright, p_validation_bright = stats.pearsonr(Y_bright_val, predicted_validation_bright)\n",
    "\n",
    "print(pearson_validation_bright)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba41673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T21:56:17.170983Z",
     "start_time": "2022-05-25T21:56:08.602556Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = x_train\n",
    "X_validate = x_validate\n",
    "Y_train = y_train\n",
    "Y_validate = y_validate\n",
    "\n",
    "predicted_metric_training = model.predict(X_train, batch_size=batch_size, verbose=0)\n",
    "predicted_metric_validation = model.predict(X_validate, batch_size = batch_size, verbose=0)\n",
    "\n",
    "predicted_training = np.reshape(predicted_metric_training, len(Y_train))\n",
    "predicted_validation = np.reshape(predicted_metric_validation, len(Y_validate))\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "pearson_training, p_training = stats.pearsonr(Y_train, predicted_training)\n",
    "pearson_validation, p_validation = stats.pearsonr(Y_validate, predicted_validation)\n",
    "\n",
    "print(\"Pearson training \"+metric_choice+\": \"+str(pearson_training)+\"\\t p_training \"+metric_choice+\": \"+str(p_training))\n",
    "print(\"Pearson validation \"+metric_choice+\": \"+str(pearson_validation)+\"\\t p_validation \"+metric_choice+\": \"+str(p_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b6634",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeba6a3",
   "metadata": {},
   "source": [
    "### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6472d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T21:56:17.465928Z",
     "start_time": "2022-05-25T21:56:17.174682Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_Metric(actual_metric_training, actual_metric_validation, predicted_metric_training, predicted_metric_validation, \\\n",
    "              metric_name_choice, output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    # summarize history for loss\n",
    "    plt.scatter(actual_metric_training, predicted_metric_training, marker = 'x', color = 'black', label='Training data')\n",
    "    plt.scatter(actual_metric_validation, predicted_metric_validation, marker = 'o', color = 'red', label='Validation data')\n",
    "    ax1.set_ylabel('Predicted ' + metric_choice, fontsize=18)\n",
    "    ax1.set_xlabel('Actual ' + metric_choice, fontsize=18)\n",
    "    \n",
    "    \n",
    "    bottom, top = plt.ylim()\n",
    "    left, right = plt.xlim()\n",
    "    lower = min(bottom, left)\n",
    "    higher = max(top, right)\n",
    "    plt.plot([lower, higher], [lower, higher], label='Ideal behaviour')\n",
    "    \n",
    "    #ax1.legend(loc='best')\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#     plt.savefig(output_folder + metric + '_graph' + currentDateTime + '_' + optimizer_choice_string + '_' + organ + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_Metric(Y_train, Y_validate, predicted_metric_training, predicted_metric_validation, metric_choice, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36606e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T21:56:18.161214Z",
     "start_time": "2022-05-25T21:56:17.549767Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_Metric_heatmap(actual_metric, predicted_metric, trainingOrValidation, metric_name_choice, output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # summarize history for loss\n",
    "#     print(predicted_metric[0].shape)\n",
    "    predicted_metric_reshaped = np.reshape(predicted_metric, actual_metric.shape)\n",
    "    h = plt.hist2d(actual_metric, predicted_metric_reshaped, 50, range = [[0,1], [0,1]], density = True, cmap='viridis')\n",
    "    plt.colorbar(h[3], ax=ax1)\n",
    "    y_axis_label = 'Predicted ' + trainingOrValidation + ' ' + metric_name_choice\n",
    "    x_axis_label = 'Actual ' + trainingOrValidation + ' ' + metric_name_choice\n",
    "    ax1.set_ylabel(y_axis_label, fontsize=18)\n",
    "    ax1.set_xlabel(x_axis_label, fontsize=18)\n",
    "\n",
    "    bottom, top = plt.ylim()\n",
    "    left, right = plt.xlim()\n",
    "    lower = min(bottom, left)\n",
    "    higher = max(top, right)\n",
    "    plt.plot([lower, higher], [lower, higher], color = 'white', label = 'Ideal behaviour')\n",
    "    leg = ax1.legend(loc='best')\n",
    "    leg.get_frame().set_alpha(0)\n",
    "    plt.setp(leg.get_texts(), color='w')\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#     plt.savefig(output_folder + metric_name_choice + '_heatmap_' + trainingOrValidation + '_' + currentDateTime + '_' \\\n",
    "#                 + optimizer_choice_string + '_' + organ + '.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_Metric_heatmap(Y_train, predicted_metric_training, \"Training\", metric_choice, output_folder)\n",
    "plot_Metric_heatmap(Y_validate, predicted_metric_validation, \"Validation\", metric_choice, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a51b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T21:56:23.234766Z",
     "start_time": "2022-05-25T21:56:22.014145Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as tik\n",
    "n_bins0 = 25\n",
    "def plot_Metric_histogram(actual_metric, predicted_metric, trainingOrValidation, metric_name_choice, output_folder):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.hist(actual_metric, bins = n_bins, range=(0,1), density = True, alpha=0.6, label='Actual ' + metric_name_choice)\n",
    "    #density = True makes the first element of the return tuple will be the counts normalized to form a probability density\n",
    "    #i.e. the area (or integral) under the histogram will sum to 1\n",
    "    plt.hist(predicted_metric, bins = n_bins, range=(0,1), density = True, alpha=0.6, label='Predicted ' + metric_name_choice)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    ax1.set_ylabel('Frequency', fontsize=18)\n",
    "    x_axis_label = trainingOrValidation + ' ' + metric_name_choice\n",
    "    ax1.set_xlabel(x_axis_label, fontsize=18)\n",
    "    #set significant figures on x axis\n",
    "    #ax1.xaxis.set_major_formatter(tik.FormatStrFormatter('%0.2f'))\n",
    "    #locx = tik.MultipleLocator(base=0.1) # this locator puts ticks at regular intervals\n",
    "    #locy = tik.MultipleLocator(base=1)\n",
    "    #ax1.xaxis.set_major_locator(locx)\n",
    "    #ax1.yaxis.set_major_locator(locy)\n",
    "#     currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "    #plt.savefig(output_folder + metric + '_histogram_' + trainingOrValidation + '_' + currentDateTime + '_' +\\\n",
    "    #            optimizer_choice_string + '_' + organ + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_Metric_histogram(Y_train, predicted_metric_training, \"Training\", metric_choice, output_folder)\n",
    "plot_Metric_histogram(Y_validate, predicted_metric_validation, \"Validation\", metric_choice, output_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7aff82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.569551Z",
     "start_time": "2022-05-15T11:12:25.343Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Y_train.shape)\n",
    "print(predicted_metric_training.shape)\n",
    "Y_train_reshaped = np.reshape(Y_train, predicted_metric_training.shape)\n",
    "print(Y_train_reshaped.shape)\n",
    "joint_train = np.hstack((Y_train_reshaped, predicted_metric_training))\n",
    "joint_dataFrame = pd.DataFrame(joint_train, columns = [\"Actual Training \" + metric, \"Predicted Training \" + metric])\n",
    "sb.set_style(\"white\")\n",
    "sb.set_style({'xtick.bottom': True,\n",
    " 'xtick.color': '.15',\n",
    " 'xtick.direction': 'out',\n",
    " 'xtick.top': False,\n",
    " 'ytick.color': '.15',\n",
    " 'ytick.direction': 'out',\n",
    " 'ytick.left': True,\n",
    " 'ytick.right': True,\n",
    " 'ytick.major.size': 05.0, \n",
    " 'xtick.major.size': 5.0})\n",
    "sb.set_context(\"notebook\", font_scale=1.5)\n",
    "sb.jointplot(x = \"Actual Training \" + metric, y = \"Predicted Training \" + metric, data=joint_dataFrame, \\\n",
    "             stat_func = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423afb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.571205Z",
     "start_time": "2022-05-15T11:12:25.346Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Y_validate.shape)\n",
    "print(predicted_metric_validation.shape)\n",
    "Y_validate_reshaped = np.reshape(Y_validate, predicted_metric_validation.shape)\n",
    "print(Y_validate_reshaped.shape)\n",
    "joint_validate = np.hstack((Y_validate_reshaped, predicted_metric_validation))\n",
    "joint_dataFrame_validate = pd.DataFrame(joint_validate, columns = [\"Actual Validation \" + metric, \\\n",
    "                                                                   \"Predicted Validation \" + metric])\n",
    "sb.set_style(\"white\")\n",
    "sb.set_style({'xtick.bottom': True,\n",
    " 'xtick.color': '.15',\n",
    " 'xtick.direction': 'out',\n",
    " 'xtick.top': False,\n",
    " 'ytick.color': '.15',\n",
    " 'ytick.direction': 'out',\n",
    " 'ytick.left': True,\n",
    " 'ytick.right': True,\n",
    " 'ytick.major.size': 05.0, \n",
    " 'xtick.major.size': 5.0})\n",
    "print(sb.axes_style())\n",
    "sb.set_context(\"notebook\", font_scale=1.5)\n",
    "sb.jointplot(x = \"Actual Validation \" + metric, y = \"Predicted Validation \" + metric, data=joint_dataFrame_validate, \\\n",
    "             stat_func = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54925a15",
   "metadata": {},
   "source": [
    "### Evaluating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac77bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T08:25:48.439317Z",
     "start_time": "2022-05-16T08:25:47.831228Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, \\\n",
    "mean_absolute_error, r2_score, max_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c696b96",
   "metadata": {},
   "source": [
    "R^2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f25b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T08:25:48.469834Z",
     "start_time": "2022-05-16T08:25:48.442683Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r2_score = r2_score(Y_train, predicted_metric_training)#, multioutput='variance_weighted') \n",
    "val_r2_score = r2_score(Y_validate, predicted_metric_validation)#, multioutput='variance_weighted')\n",
    "\n",
    "print(\"train_r2_score: \", train_r2_score)\n",
    "print(\"val_r2_score: \", val_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b712d86",
   "metadata": {},
   "source": [
    "Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f00b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T08:26:11.077952Z",
     "start_time": "2022-05-16T08:25:48.473096Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mae = mean_absolute_error(Y_train, predicted_metric_training)#, multioutput='variance_weighted') \n",
    "val_mae = mean_absolute_error(Y_validate, predicted_metric_validation)#, multioutput='variance_weighted')\n",
    "std_train_mae = np.std(np.abs(Y_train-predicted_metric_training))\n",
    "std_val_mae = np.std(np.abs(Y_validate-predicted_metric_validation))\n",
    "print(\"train_mae ± std: {} ± {}\".format(train_mae, std_train_mae))\n",
    "print(\"val_mae ± std: {} ± {}\".format(val_mae, std_val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dda045",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ad861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.577384Z",
     "start_time": "2022-05-15T11:12:25.359Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mape = mean_absolute_percentage_error(Y_train, predicted_metric_training)#, multioutput='variance_weighted') \n",
    "val_mape = mean_absolute_percentage_error(Y_validate, predicted_metric_validation)#, multioutput='variance_weighted')\n",
    "\n",
    "print(\"train_mape: \", train_mape)\n",
    "print(\"val_mape: \", val_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8806c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T08:26:11.340657Z",
     "start_time": "2022-05-16T08:26:11.080647Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_scores_df = pd.DataFrame([(pearson_training), (pearson_validation),\n",
    "                                 (train_r2_score), (val_r2_score),\n",
    "                                 (train_mae), (val_mae),\n",
    "                                (std_train_mae), (std_val_mae)],\n",
    "                               columns=[metric_choice],\n",
    "                               index=[\"Pearson training\", \"Pearson validation\",\n",
    "                                        \"r^2 score training\", \"r^2 score validation\",\n",
    "                                        \"MAE training\", \"MAE validation\",\n",
    "                                     \"MAE training std\", \"MAE validation std\"])\n",
    "\n",
    "# print(metric_scores_df)\n",
    "\n",
    "fig = plt.figure(figsize = (8, 2))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.table(cellText = metric_scores_df.values.round(decimals=4),\n",
    "         rowLabels = metric_scores_df.index,\n",
    "         colLabels = metric_scores_df.columns,\n",
    "         cellLoc=\"center\", loc = \"top\",\n",
    "         bbox=[0, -0.55, 0.4, 1.5] \n",
    "         )\n",
    "\n",
    "ax.set_title(metric_choice+\" network\", loc=\"left\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ef105",
   "metadata": {},
   "source": [
    "Group the metric arrays by intervals of width 0.1 for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38e147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.580332Z",
     "start_time": "2022-05-15T11:12:25.363Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_by_interval(dataframe):\n",
    "    lb, ub, metric_idx, sort_metric_idx, sort_idx=[],[],[],[],[]\n",
    "    for i in range(10):\n",
    "        lb.append(str(i/10)); ub.append(str(0.1+i/10))\n",
    "        metric_idx.append(lb[i]+' < '+metric+' <= '+ub[i])\n",
    "        sort_idx.append(dataframe.query(metric_idx[i]).index.to_numpy(dtype=int))\n",
    "    sort_metric_idx = np.array(sort_idx, dtype=object)\n",
    "#     print(sort_metric_idx)\n",
    "    return sort_metric_idx\n",
    "\n",
    "# sort_by_interval(pred_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf532fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.581771Z",
     "start_time": "2022-05-15T11:12:25.366Z"
    }
   },
   "outputs": [],
   "source": [
    "def boxplot(metric_array, predicted_metric, dataframe, metric_name_choice, TorV, absolute=None):\n",
    "    data=[]\n",
    "    interval_number=10; n=100\n",
    "    null_intervals=False\n",
    "    for i in range(interval_number):\n",
    "        error_array=error(metric_array, predicted_metric, absolute=absolute)\n",
    "        interval_data=error_array[sort_by_interval(dataframe)[i]]\n",
    "#         interval_data=interval_data/len(interval_data)\n",
    "        if 0<len(interval_data)<n :\n",
    "            print('Interval '+str(i+1)+' contains '+str(len(interval_data))+' data points. Change the interval widths'\\\n",
    "                  ' to not have small datasets.')\n",
    "        if len(interval_data)<=1:\n",
    "            null_intervals=True; null_intervals_idx=[]\n",
    "            null_intervals_idx.append(i)\n",
    "            del interval_data\n",
    "        else:\n",
    "            data.append(interval_data)\n",
    "    fig, ax0 = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax0.boxplot(data)\n",
    "    ax0.set_title(metric_name_choice+' '+TorV)\n",
    "    ax0.set_ylabel('Absolute error density')\n",
    "    ax0.set_xlabel(metric_name_choice+' intervals')\n",
    "    if null_intervals==False:\n",
    "        plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['(0,0.1]','(0.1,0.2]','(0.2,0.3]','(0.3,0.4]','(0.4,0.5]',\n",
    "                    '(0.5,0.6]','(0.6,0.7]','(0.7,0.8]','(0.8,0.9]','(0.9,1]'], fontsize=9.5)\n",
    "    if null_intervals==True:\n",
    "        plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9], ['(0,0.2]','(0.2,0.3]','(0.3,0.4]','(0.4,0.5]',\n",
    "                    '(0.5,0.6]','(0.6,0.7]','(0.7,0.8]','(0.8,0.9]','(0.9,1]'], fontsize=9.5)\n",
    "    ax0.set_ylim(0,0.16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "boxplot(Y_train, predicted_metric_training, pred_train_df, metric_name, 'training', absolute=True)\n",
    "boxplot(Y_validate, predicted_metric_validation, pred_val_df, metric_name, 'validation', absolute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a7312",
   "metadata": {},
   "source": [
    "### Error Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa197ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T08:26:42.807427Z",
     "start_time": "2022-05-16T08:26:42.798098Z"
    }
   },
   "outputs": [],
   "source": [
    "def error(metric_array, metric_prediction, absolute=None):   \n",
    "    temp=[]\n",
    "    sq_metric_prediction = np.squeeze(metric_prediction)\n",
    "    for i in range(len(metric_array)):\n",
    "        if absolute==True:\n",
    "            error = np.abs(metric_array[i] - sq_metric_prediction[i])\n",
    "        if absolute==False:\n",
    "            error = metric_array[i] - sq_metric_prediction[i]\n",
    "        temp.append(error)\n",
    "        error_array = np.array(temp)\n",
    "    return error_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550df49d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.584754Z",
     "start_time": "2022-05-15T11:12:25.371Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_val_df = pd.DataFrame({metric+' predicted validation': np.squeeze(predicted_metric_validation),\n",
    "                            metric: Y_validate,\n",
    "                            metric+' absolute error': error(Y_validate, predicted_metric_validation, absolute=True)},\n",
    "                           index=np.arange(len(predicted_metric_validation)),\n",
    "                           columns=[metric+' predicted validation', metric,\n",
    "                                    metric+' absolute error'])\n",
    "\n",
    "pred_train_df = pd.DataFrame({metric+' predicted training': np.squeeze(predicted_metric_training),\n",
    "                              metric: Y_train,\n",
    "                              metric+' absolute error': error(Y_train, predicted_metric_training, absolute=True)},\n",
    "                             index=np.arange(len(predicted_metric_training)),\n",
    "                             columns=[metric+' predicted training', metric,\n",
    "                                      metric+' absolute error'])\n",
    "\n",
    "print(pred_val_df)\n",
    "print(pred_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb85bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.586239Z",
     "start_time": "2022-05-15T11:12:25.373Z"
    }
   },
   "outputs": [],
   "source": [
    "def AE_metric_plot(metric_array, metric_prediction, metric_name_choice, TorV):\n",
    "    fig, ax0 = plt.subplots()\n",
    "    error_array = error(metric_array, metric_prediction, absolute=True)\n",
    "    plt.scatter(metric_array, error_array)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax0.set_xlabel(metric_name_choice+\" \"+TorV+' score', fontsize=18)\n",
    "    ax0.set_ylabel('Absolute error', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "#     ax0.set_xlim(-0.01, 1.01)\n",
    "#     ax0.set_ylim(-0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722638bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T10:28:51.321250Z",
     "start_time": "2022-05-16T10:28:49.277075Z"
    }
   },
   "outputs": [],
   "source": [
    "def error_hist(metric_array, metric_prediction, metric_name_choice, TorV):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3.5))\n",
    "#     absolute lets you choose if you want the error or the absolute error \n",
    "    error_array = error(metric_array, metric_prediction, absolute=True) \n",
    "    print('mean ± std MAE: {} ± {}'.format(np.mean(error_array), np.std(error_array)))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    metric_freq_histog, bin_edges = np.histogram(metric_array, bins=40)\n",
    "    print(metric_freq_histog)\n",
    "    plt.hist()\n",
    "    plt.hist(error_array, density = False, bins = 40)#200, range = (-1,1))\n",
    "    ax1.set_ylabel('Frequency', fontsize=18)\n",
    "    ax1.set_xlabel(metric_choice + \" \" + TorV + ' absolute error', fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "#         totalNumberSlices = str(metric_array[i].shape[0])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# error_hist(Y_train, predicted_metric_training, metric_name, 'training')\n",
    "# error_hist(Y_validate, predicted_metric_validation, metric_name, 'validation')\n",
    "\n",
    "def error_freq_hist(metric_array, metric_prediction, metric_name_choice, TorV, Nbins):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3.5))\n",
    "#     absolute lets you choose if you want the error or the absolute error \n",
    "    error_array = error(metric_array, metric_prediction, absolute=True) \n",
    "    print('mean ± std MAE: {} ± {}'.format(np.mean(error_array), np.std(error_array)))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    metric_freq_histog, bin_edges = np.histogram(metric_array, bins=Nbins)\n",
    "    print(len(metric_freq_histog))\n",
    "    print(len(bin_edges))\n",
    "    MAEs=np.empty((len(metric_freq_histog)))\n",
    "    \n",
    "    cumu_sum=metric_freq_histog[0]\n",
    "    for i in range(len(MAEs)):\n",
    "        bin_i = metric_freq_histog[i]\n",
    "        bin_before_i = metric_freq_histog[i-1]\n",
    "        if i==0:\n",
    "            MAEs[i] = np.max(error_array[:cumu_sum])\n",
    "        else:\n",
    "            MAEs[i] = a\n",
    "            cumu_sum+=bin_i\n",
    "    ax1.scatter(bin_edges[:-1], MAEs)\n",
    "    plt.xlim(0,1)\n",
    "    ax1.set_ylabel('Dice validation MAE', fontsize=18)\n",
    "    ax1.set_xlabel(metric_choice + ' score', fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "error_freq_hist(Y_validate[:9326], predicted_metric_validation[:9326], metric_name, 'validation', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973060e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T09:21:35.990232Z",
     "start_time": "2022-05-16T09:21:33.828900Z"
    }
   },
   "outputs": [],
   "source": [
    "def error_metric_hist2d(metric_array, metric_prediction, metric_name_choice, TorV, bin_x, bin_y):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    n_binsx = bin_x; n_binsy=bin_y\n",
    "#     absolute lets you choose if you want the error or the absolute error \n",
    "    error_array = error(metric_array, metric_prediction, absolute=True) \n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.hist2d(metric_array, error_array, bins=(n_binsx, n_binsy), cmap=plt.cm.jet)\n",
    "    ax1.set_xlabel(metric_name_choice+' score', fontsize=18)\n",
    "    ax1.set_ylabel(metric_name_choice + \" \" + TorV + ' absolute error', fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.colorbar(label='Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "error_metric_hist2d(Y_validate[:9326], predicted_metric_validation[:9326], 'Dice', 'validation', 30, 30)\n",
    "# error_metric_hist2d(Y_train1, predicted_metric_training[0], metric_name[0], 'training')\n",
    "# error_metric_hist2d(Y_validate1, predicted_metric_validation[0], metric_name[0], 'validation')\n",
    "# error_metric_hist2d(Y_train2, predicted_metric_training[1], metric_name[1], 'training')\n",
    "# error_metric_hist2d(Y_validate2, predicted_metric_validation[1], metric_name[1], 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f4702",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### More Prediction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97536e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.590530Z",
     "start_time": "2022-05-15T11:12:25.380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "#Calculate mean and largest deviations of prediction from actual value\n",
    "\n",
    "#The prediction arrays have shape (length, 1). We want to reshape them to just (length,)\n",
    "predicted_metric_training_newshape = np.reshape(predicted_metric_training, len(Y_train))\n",
    "predicted_metric_validation_newshape = np.reshape(predicted_metric_validation, len(Y_validate))\n",
    "mean_difference_training = np.sum(abs(predicted_metric_training_newshape - Y_train))/len(Y_train)\n",
    "largest_difference_training = abs(predicted_metric_training_newshape - Y_train).max()\n",
    "mean_difference_validation = np.sum(abs(predicted_metric_validation_newshape - Y_validate))/len(Y_validate)\n",
    "largest_difference_validation = abs(predicted_metric_validation_newshape - Y_validate).max()\n",
    "print('Average absolute difference in '+ metric+', training data : ')\n",
    "print(mean_difference_training)\n",
    "print('Average absolute difference in '+ metric+', validation data : ')\n",
    "print(mean_difference_validation)\n",
    "print('Largest absolute difference in '+ metric+', training data : ')\n",
    "print(largest_difference_training)\n",
    "print('Largest absolute difference in '+ metric+', validation data : ')\n",
    "print(largest_difference_validation)\n",
    "\n",
    "#Calculate percentiles\n",
    "q = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 96, 97, 98, 99, 100]\n",
    "training_percentiles = np.percentile(abs(predicted_metric_training_newshape - Y_train), q)\n",
    "print('The percentiles for the training data are:')\n",
    "for i in range(0, len(q)):\n",
    "    print(str(q[i]) + '%: ' + str(training_percentiles[i]))\n",
    "\n",
    "validation_percentiles = np.percentile(abs(predicted_metric_validation_newshape - Y_validate), q)\n",
    "print('\\nThe percentiles for the validation data are:')\n",
    "for i in range(0, len(q)):\n",
    "    print(str(q[i]) + '%: ' + str(validation_percentiles[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d023d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.591923Z",
     "start_time": "2022-05-15T11:12:25.383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(output_folder+'deviations_from_actual_values.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f72ad0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Prediction - Actual DICE score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f7357",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767f8b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.593268Z",
     "start_time": "2022-05-15T11:12:25.386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_metric_difference = predicted_metric_training_newshape - Y_train\n",
    "fig, ax1 = plt.subplots()\n",
    "#removes the grey background to the plot\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.hist(training_metric_difference, density = False, bins = 200, range = (-1,1))\n",
    "ax1.set_ylabel('Frequency', fontsize=18)\n",
    "ax1.set_xlabel('Predicted Training '+metric+' - Actual Training '+metric, fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "plt.savefig(output_folder + 'training_'+metric+'_difference_histogram_' + organ + '_' + currentDateTime + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ea8d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef6a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.594667Z",
     "start_time": "2022-05-15T11:12:25.389Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Plot histogram of predicted - actual Dice score for validation data\n",
    "validation_metric_difference = predicted_metric_validation_newshape - Y_validate\n",
    "fig, ax1 = plt.subplots()\n",
    "#removes the grey background to the plot\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.hist(validation_metric_difference, density = False, bins = 200, range = (-1,1))\n",
    "ax1.set_ylabel('Frequency', fontsize=18)\n",
    "ax1.set_xlabel('Predicted Validation '+metric+' - Actual Validation '+metric, fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "currentDateTime = datetime.datetime.now().strftime(\"%H-%M_%B-%d-%Y\")\n",
    "#plt.savefig(output_folder + 'validation_'+metric+'_difference_histogram_' + organ + '_' + currentDateTime + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25412c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3926e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596115e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below will follow:\n",
    "\n",
    "* Dice to 3D MSD correspondence\n",
    "* Deviations against actual values of metric\n",
    "* False positives and negatives\n",
    "These are useful to analyse which is the best threshold value of the surface distance metric. Any registrations with predicted value above the threshold would be flagged for manual review in a clinical setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b5cfc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Calculate Dice score to 3D MSD correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0dc724",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### With a Single 3D MSD Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f454b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.596022Z",
     "start_time": "2022-05-15T11:12:25.394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "from scipy import stats import decimal\n",
    "\n",
    "x_axis = [] corresponding_mean_dice = [] corresponding_mean_error = [] number_of_datapoints = []\n",
    "\n",
    "dice_array = multiple_dice_array[np.where(multiple_dice_array != 0)] msd_array = metric_array\n",
    "\n",
    "if len(dice_array) != len(msd_array): raise ValueError(\"The dice and msd arrays did not have the same length.\")\n",
    "\n",
    "actual_value = decimal.Decimal('0.0') while (actual_value < decimal.Decimal('1.0')):\n",
    "```\n",
    "----------------\n",
    "```\n",
    "indices = np.where((actual_value <= msd_array) & (msd_array < actual_value + decimal.Decimal('0.1')))\n",
    "\n",
    "print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('0.1')) + \": \" + str(len(indices[0])))\n",
    "number_of_datapoints.append(len(indices[0]))\n",
    "\n",
    "corresponding_mean_dice.append(np.mean(dice_array[indices]))\n",
    "sd = np.std(dice_array[indices])\n",
    "corresponding_mean_error.append(sd)#/(len(dice_array[indices])-1)**0.5) #stats.sem(dice_array[indices]))\n",
    "\n",
    "x_axis.append(actual_value+decimal.Decimal('0.05'))\n",
    "\n",
    "actual_value += decimal.Decimal('0.1')\n",
    "```\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62628a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### With Two 3D MSD Arrays at the Same Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38252aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.597474Z",
     "start_time": "2022-05-15T11:12:25.397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "import decimal\n",
    "\n",
    "x_axis = [] corresponding_mean_dice = [] corresponding_mean_error = [] number_values = [] corresponding_mean_dice_transformed = [] corresponding_mean_error_transformed = [] number_values_transformed = []\n",
    "\n",
    "dice_array = multiple_dice_array[np.where(multiple_dice_array != 0)] msd_array_transformed = metric_array msd_array = msd_no_zero_dice_normalised\n",
    "\n",
    "if (len(dice_array) != len(msd_array) or len(dice_array) != len(msd_array_transformed)): raise ValueError(\"The dice and msd arrays did not have the same length.\")\n",
    "\n",
    "actual_value = decimal.Decimal('0.0') while (actual_value < decimal.Decimal('1.0')):\n",
    "```\n",
    "--------------------\n",
    "```\n",
    "indices_transformed = np.where((actual_value <= msd_array_transformed) & (msd_array_transformed < actual_value + decimal.Decimal('0.1')))\n",
    "indices = np.where((actual_value <= msd_array) & (msd_array < actual_value + decimal.Decimal('0.1')))\n",
    "\n",
    "print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('0.1')) + \": \" + str(len(indices[0])) + \" \" + str(len(indices_transformed[0])))\n",
    "number_values.append(len(indices[0]))\n",
    "number_values_transformed.append(len(indices_transformed[0]))\n",
    "\n",
    "corresponding_mean_dice_transformed.append(np.mean(dice_array[indices_transformed]))\n",
    "corresponding_mean_error_transformed.append(np.std(dice_array[indices_transformed]))\n",
    "\n",
    "corresponding_mean_dice.append(np.mean(dice_array[indices]))\n",
    "corresponding_mean_error.append(np.std(dice_array[indices]))\n",
    "\n",
    "x_axis.append(actual_value+decimal.Decimal('0.05'))\n",
    "\n",
    "actual_value += decimal.Decimal('0.1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1c407",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting Functions for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533434a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.598885Z",
     "start_time": "2022-05-15T11:12:25.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white')\n",
    "\n",
    "h = plt.hist2d(metric_array, dice_array, 50, range = [[0,1], [0,1]], density = True, cmap='viridis') plt.colorbar(h[3], ax=ax1) y_axis_label = 'Dice score' x_axis_label = '3D MSD fourth-root' ax1.set_ylabel(y_axis_label, fontsize=18) ax1.set_xlabel(x_axis_label, fontsize=18)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e171dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.600204Z",
     "start_time": "2022-05-15T11:12:25.402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l2 = ax1.errorbar(x_axis, corresponding_mean_dice, corresponding_mean_error, color='b', label=\"Normal\") l1 = ax1.errorbar(x_axis, corresponding_mean_dice_transformed, corresponding_mean_error_transformed, color='g', label=\"Fourth-root\")\n",
    "\n",
    "l3 = ax1.scatter(x_axis, corresponding_mean_dice, s = 0.1np.array(number_values), color='r') l4 = ax1.scatter(x_axis, corresponding_mean_dice_transformed, s = 0.1np.array(number_values_transformed), color='r')\n",
    "\n",
    "ax1.set_ylabel('Mean Dice score', fontsize=18) ax1.set_xlabel('3D MSD', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8)\n",
    "\n",
    "#Colour legend markers = [l2, l1] #labels = [\"Normal\", \"Fourth-root\"] fig.legend(handles=markers, loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#Size legend handles, labels = l3.legend_elements(prop=\"sizes\", num = [100,300,500,700], alpha=0.5, color='red') ax1.legend(handles, labels, loc=\"best\", title=\"#Datapoints\", numpoints = 1, labelspacing=1) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.ylim(0,1)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0,1) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4134947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.601509Z",
     "start_time": "2022-05-15T11:12:25.405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "ig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.errorbar(metric_array, dice_array, fmt='.', color='blue') ax1.set_ylabel('Dice score', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) start, end = ax1.get_xlim() ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.ylim(-0.01,1)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) #plt.xlim(-0.05,0.95) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82cde9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Investigate deviations/error against actual values of the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3aa1db",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Absolute Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510582a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`predicted_training = np.reshape(predicted_metric_training, len(Y_train)) predicted_validation = np.reshape(predicted_metric_validation, len(Y_validate))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba231ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.602952Z",
     "start_time": "2022-05-15T11:12:25.409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "```\n",
    "import decimal from scipy import stats #To use stats.sem() to calculate the standard error in the mean of the values of an array\n",
    "\n",
    "x_axis = [] all_mean_deviations_training = [] all_errors_mean_training = []\n",
    "\n",
    "all_mean_deviations_validation = [] all_errors_mean_validation = []\n",
    "\n",
    "all_percentile_95_deviations_training = [] all_percentile_95_deviations_validation = []\n",
    "\n",
    "actual_value = decimal.Decimal('0.1') while (actual_value < decimal.Decimal('1.0')):\n",
    "    \n",
    "```\n",
    "-------------------\n",
    "```\n",
    "indices_training = np.where((actual_value <= Y_train) & (Y_train < actual_value + decimal.Decimal('0.05')))\n",
    "indices_validation = np.where((actual_value <= Y_validate) & (Y_validate < actual_value + decimal.Decimal('0.05')))\n",
    "\n",
    "print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('0.05')) + \": \" + str(len(indices_training[0])) + \" \" + str(len(indices_validation[0])))\n",
    "\n",
    "all_mean_deviations_training.append(np.mean(abs(predicted_training[indices_training] - Y_train[indices_training])))\n",
    "all_errors_mean_training.append(np.std(abs(predicted_training[indices_training] - Y_train[indices_training])))\n",
    "\n",
    "all_mean_deviations_validation.append(np.mean(abs(predicted_validation[indices_validation] - Y_validate[indices_validation])))\n",
    "all_errors_mean_validation.append(np.std(abs(predicted_validation[indices_validation] - Y_train[indices_validation])))\n",
    "\n",
    "all_percentile_95_deviations_training.append(np.percentile(abs(predicted_training[indices_training] - Y_train[indices_training]), 95))\n",
    "all_percentile_95_deviations_validation.append(np.percentile(abs(predicted_validation[indices_validation] - Y_validate[indices_validation]), 95))\n",
    "\n",
    "x_axis.append(actual_value + decimal.Decimal('0.025'))\n",
    "\n",
    "actual_value += decimal.Decimal('0.05')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73004ea",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting Code for above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae019a8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') #l1 = ax1.errorbar(x_axis, all_mape_training, all_errorbars_mape_training, color='cornflowerblue', label=\"Training\") l2 = ax1.errorbar(x_axis, all_mean_deviations_validation, all_errors_mean_validation, color='g', label=\"Validation\")\n",
    "\n",
    "#l3 = ax1.scatter(x_axis, all_mape_training, s = 0.1*np.array(datapoints_training), color='r') l4 = ax1.scatter(x_axis, all_mean_deviations_validation, s = np.array(datapoints_validation), color='r')\n",
    "\n",
    "#Colour legend #fig.legend(handles=[l1,l2], loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#Size legend handles, labels = l4.legend_elements(prop=\"sizes\", num = [100,250,500,1000], alpha = 0.5, color='red') ax1.legend(handles, labels, loc=\"best\", title=\"#Datapoints\", numpoints = 1, labelspacing=1.1)\n",
    "\n",
    "ax1.set_ylabel('Mean Absolute Error', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=15) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) #plt.grid() plt.ylim(0,0.5)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0.1,1) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821f0b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') #l1 = ax1.errorbar(x_axis, all_mape_training, all_errorbars_mape_training, color='cornflowerblue', label=\"Training\") #l2 = ax1.errorbar(x_axis, all_mape_validation, all_errorbars_mape_validation, color='g', label=\"Validation\")\n",
    "\n",
    "#l3 = ax1.scatter(x_axis, all_mape_training, s = 0.1*np.array(datapoints_training), color='r') l4 = ax1.scatter(x_axis, all_mean_deviations_validation, marker='x', s=500, color='r', linewidth=3)\n",
    "\n",
    "#Colour legend #fig.legend(handles=l2, loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#ax1.set_ylabel('Mean Absolute Percentage Error', fontsize=18) #ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=25) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) ax1.xaxis.tick_top() ax1.yaxis.tick_right() ax1.yaxis.set_ticks(np.arange(0.02, 0.051, 0.005)) plt.grid() plt.ylim(0.02,0.053)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0.2,0.5) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4556861",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.errorbar(x_axis, all_mean_deviations_training, all_errors_mean_training, fmt='.', color='cornflowerblue', label='Training') l2 = ax1.errorbar(x_axis, all_mean_deviations_validation, all_errors_mean_validation, fmt='.', color='green', label='Validation') ax1.set_ylabel('Mean absolute deviation', fontsize=18) ax1.set_xlabel('Actual value', fontsize=18) ax1.legend(loc='best', fontsize=14) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.ylim(0,0.12) #np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0.1,1) plt.tight_layout()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3a22",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.scatter(x_axis, all_percentile_95_deviations_training,color='cornflowerblue', label='Training deviation') l2 = ax1.scatter(x_axis, all_percentile_95_deviations_validation,color='red', label='Validation deviation') ax1.set_ylabel('95 percentile deviation', fontsize=18) ax1.set_xlabel('Actual value', fontsize=18) ax1.legend(loc='best', fontsize=14) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #plt.ylim(0,np.array([all_percentile_95_deviations_training,all_percentile_95_deviations_validation]).max()+0.005) plt.xlim(0.1,1) plt.tight_layout()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebacc2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19310c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Eliminate all actual values of 0 since would give nan when dividing predicted_training_nozero = predicted_training[np.where(Y_train != 0)] predicted_validation_nozero = predicted_validation[np.where(Y_validate != 0)]\n",
    "\n",
    "actual_train = Y_train[np.where(Y_train != 0)] actual_validate = Y_validate[np.where(Y_validate != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc44b8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "print(\"Actual train, actual validate, predictions train, predictions validate\") print(\"Shapes:\\n\", actual_train.shape, actual_validate.shape, predicted_training_nozero.shape, predicted_validation_nozero.shape) print(\"Maxima:\\n\",actual_train.max(), actual_validate.max(), predicted_training_nozero.max(), predicted_validation_nozero.max()) print(\"Minima:\\n\",actual_train.min(), actual_validate.min(), predicted_training_nozero.min(), predicted_validation_nozero.min())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7db624",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Work with 3D MSD by transforming back from 3D MSD fourth-root min_value_after_first_root = 0.22832586673831098 max_value_after_first_root = 2.7136906443840783\n",
    "```\n",
    "predicted_training_nozero = ((max_value_after_first_root - min_value_after_first_root)predicted_training_nozero*2 + min_value_after_first_root)2 predicted_validation_nozero = ((max_value_after_first_root - min_value_after_first_root)*predicted_validation_nozero2 + min_value_after_first_root)**2\n",
    "\n",
    "actual_train = ((max_value_after_first_root - min_value_after_first_root)actual_train*2 + min_value_after_first_root)2 actual_validate = ((max_value_after_first_root - min_value_after_first_root)*actual_validate2 + min_value_after_first_root)**2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e38799",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "import decimal from scipy import stats #To use stats.sem() which provides the standard error in the mean of the values of an array\n",
    "\n",
    "x_axis = [] all_mape_training = [] all_errorbars_mape_training = [] datapoints_training = [] all_mape_validation = [] all_errorbars_mape_validation = [] datapoints_validation = []\n",
    "\n",
    "if (len(actual_train) != len(predicted_training_nozero) or len(actual_validate) != len(predicted_validation_nozero)): raise ValueError(\"The arrays of actual values and predictions did not have the same shape!\")\n",
    "\n",
    "actual_value = decimal.Decimal('0.0') while (actual_value < decimal.Decimal('1.0')):\n",
    "``` \n",
    "-------------------------------------\n",
    "```\n",
    "indices_training = np.where((actual_value <= actual_train) & (actual_train < actual_value + decimal.Decimal('0.1')))\n",
    "    indices_validation = np.where((actual_value <= actual_validate) & (actual_validate < actual_value + decimal.Decimal('0.1')))\n",
    "    \n",
    "    print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('0.1')) + \": \" + str(len(indices_training[0])) + \" \" + str(len(indices_validation[0])))\n",
    "    \n",
    "    mape_training = np.mean(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/actual_train[indices_training])\n",
    "    all_errorbars_mape_training.append(np.std(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/actual_train[indices_training]))\n",
    "    all_mape_training.append(mape_training)\n",
    "    datapoints_training.append(len(indices_training[0]))\n",
    "    \n",
    "    mape_validation = np.mean(abs(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/actual_validate[indices_validation])\n",
    "    all_errorbars_mape_validation.append(np.std(abs(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/actual_validate[indices_validation]))\n",
    "    all_mape_validation.append(mape_validation)\n",
    "    datapoints_validation.append(len(indices_validation[0]))\n",
    "    \n",
    "    x_axis.append(actual_value+decimal.Decimal('0.05'))\n",
    "    \n",
    "    actual_value += decimal.Decimal('0.1')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56e724",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b42fe1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') #l1 = ax1.errorbar(x_axis, all_mape_training, all_errorbars_mape_training, color='cornflowerblue', label=\"Training\") l2 = ax1.errorbar(x_axis, all_mape_validation, all_errorbars_mape_validation, color='g', label=\"Validation\")\n",
    "\n",
    "#l3 = ax1.scatter(x_axis, all_mape_training, s = 0.1np.array(datapoints_training), color='r') l4 = ax1.scatter(x_axis, all_mape_validation, s = 0.5np.array(datapoints_validation), color='r')\n",
    "\n",
    "#Colour legend #fig.legend(handles=[l1,l2], loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#Size legend handles, labels = l4.legend_elements(prop=\"sizes\", num = [50,100,250,500,1000], alpha = 0.5, color='red') ax1.legend(handles, labels, loc=\"best\", title=\"#Datapoints\", numpoints = 1, labelspacing=1)\n",
    "\n",
    "ax1.set_ylabel('Mean Absolute Percentage Error', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=15) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.grid() plt.ylim(0,0.6)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0,1) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459be609",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') #l1 = ax1.errorbar(x_axis, all_mape_training, all_errorbars_mape_training, color='cornflowerblue', label=\"Training\") #l2 = ax1.errorbar(x_axis, all_mape_validation, all_errorbars_mape_validation, color='g', label=\"Validation\")\n",
    "\n",
    "#l3 = ax1.scatter(x_axis, all_mape_training, s = 0.1*np.array(datapoints_training), color='r') l4 = ax1.scatter(x_axis, all_mape_validation, marker='x', s=500, color='r', linewidth=3)\n",
    "\n",
    "#Colour legend #fig.legend(handles=l2, loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#ax1.set_ylabel('Mean Absolute Percentage Error', fontsize=18) #ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=25) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) ax1.xaxis.tick_top() ax1.yaxis.tick_right() #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.grid() plt.ylim(0.025,0.14)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0.25,0.71) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5529e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.errorbar(Y_validate, abs(Y_validate-predicted_validation)/Y_validate, fmt='.', color='blue') ax1.set_ylabel('APE', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) start, end = ax1.get_xlim() ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.ylim(-0.01,1)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) #plt.xlim(-0.05,0.95) plt.tight_layout()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a13976",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Symmetric Mean Absolute Percentage Error (SMAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7ba27",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Eliminate all entries at which both actual value and prediction are 0 since would give nan when dividing unwanted_training_entries = np.where((Y_train == 0) & (predicted_training == 0)) unwanted_validation_entries = np.where((Y_validate == 0) & (predicted_validation == 0))\n",
    "```\n",
    "predicted_training_nozero = np.delete(predicted_training, unwanted_training_entries) predicted_validation_nozero = np.delete(predicted_validation, unwanted_validation_entries)\n",
    "actual_training = np.delete(Y_train, unwanted_training_entries) actual_validate = np.delete(Y_validate, unwanted_validation_entries)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb141d55",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "x_axis = [] all_smape_training = [] all_errorbars_smape_training = [] all_smape_validation = [] all_errorbars_smape_validation = []\n",
    "\n",
    "if (len(actual_train) != len(predicted_training_nozero) or len(actual_validate) != len(predicted_validation_nozero)): raise ValueError(\"The arrays of actual values and predictions did not have the same shape!\")\n",
    "\n",
    "actual_value = decimal.Decimal('0.0') while (actual_value < decimal.Decimal('1.0')):\n",
    "```\n",
    "-------------\n",
    "```\n",
    "indices_training = np.where((actual_value <= actual_train) & (actual_train < actual_value + decimal.Decimal('0.1')))\n",
    "indices_validation = np.where((actual_value <= actual_validate) & (actual_validate < actual_value + decimal.Decimal('0.1')))\n",
    "\n",
    "print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('1')) + \": \" + str(len(indices_training[0])) + \" \" + str(len(indices_validation[0])))\n",
    "\n",
    "all_smape_training.append(np.mean(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/(abs(actual_train[indices_training])+abs(predicted_training_nozero[indices_training]))))\n",
    "all_errorbars_smape_training.append(stats.sem(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/(abs(actual_train[indices_training])+abs(predicted_training_nozero[indices_training]))))\n",
    "\n",
    "all_smape_validation.append(np.mean(abs(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/(abs(actual_validate[indices_validation])+abs(predicted_validation_nozero[indices_validation]))))\n",
    "all_errorbars_smape_validation.append(stats.sem(abs(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/(abs(actual_validate[indices_validation])+abs(predicted_validation_nozero[indices_validation]))))\n",
    "\n",
    "x_axis.append(actual_value+decimal.Decimal('0.05'))\n",
    "\n",
    "actual_value += decimal.Decimal('0.1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2f77b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.errorbar(x_axis, all_smape_training, all_errorbars_smape_training, fmt='.',color='cornflowerblue', label=\"Training\") l2 = ax1.errorbar(x_axis, all_smape_validation, all_errorbars_smape_validation, fmt='.',color='g', label=\"Validation\") ax1.set_ylabel('Symmetric Mean Absolute Percentage Error', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.legend(loc='best', fontsize=14, numpoints=1) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) #plt.ylim(0,0.15)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) #plt.xlim(0.1,1) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ce125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.604274Z",
     "start_time": "2022-05-15T11:12:25.423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# THE REST OF THE CODE IS STILL WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6880c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Mean Arctangent Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2408e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "#Eliminate all entries at which both actual value and prediction are 0 since would give nan when dividing unwanted_training_entries = np.where((Y_train == 0) & (predicted_training == 0)) unwanted_validation_entries = np.where((Y_validate == 0) & (predicted_validation == 0))\n",
    "\n",
    "predicted_training_nozero = np.delete(predicted_training, unwanted_training_entries) predicted_validation_nozero = np.delete(predicted_validation, unwanted_validation_entries)\n",
    "\n",
    "actual_training = np.delete(Y_train, unwanted_training_entries) actual_validate = np.delete(Y_validate, unwanted_validation_entries)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c041bd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "x_axis = [] all_maape_training = [] all_errorbars_maape_training = [] all_maape_validation = [] all_errorbars_maape_validation = []\n",
    "\n",
    "if (len(actual_train) != len(predicted_training_nozero) or len(actual_validate) != len(predicted_validation_nozero)): raise ValueError(\"The arrays of actual values and predictions did not have the same shape!\")\n",
    "\n",
    "actual_value = decimal.Decimal('0.0') while (actual_value < decimal.Decimal('1.0')):\n",
    "```\n",
    "\n",
    "---------------\n",
    "\n",
    "```\n",
    "indices_training = np.where((actual_value <= actual_train) & (actual_train < actual_value + decimal.Decimal('0.1')))\n",
    "indices_validation = np.where((actual_value <= actual_validate) & (actual_validate < actual_value + decimal.Decimal('0.1')))\n",
    "\n",
    "print(str(actual_value) + \" to \" + str(actual_value + decimal.Decimal('1')) + \": \" + str(len(indices_training[0])) + \" \" + str(len(indices_validation[0])))\n",
    "\n",
    "all_maape_training.append(np.mean(np.arctan(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/actual_train[indices_training])))\n",
    "all_errorbars_maape_training.append(stats.sem(np.arctan(abs(actual_train[indices_training]-predicted_training_nozero[indices_training])/actual_train[indices_training])))\n",
    "\n",
    "all_maape_validation.append(np.mean(abs(np.arctan(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/actual_validate[indices_validation])))\n",
    "all_errorbars_maape_validation.append(stats.sem(np.arctan(abs(actual_validate[indices_validation]-predicted_validation_nozero[indices_validation])/actual_validate[indices_validation])))\n",
    "\n",
    "x_axis.append(actual_value+decimal.Decimal('0.05'))\n",
    "\n",
    "actual_value += decimal.Decimal('0.1')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bc27c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.errorbar(x_axis, all_maape_training, all_errorbars_maape_training, fmt='.',color='cornflowerblue', label=\"Training\") l2 = ax1.errorbar(x_axis, all_maape_validation, all_errorbars_maape_validation, fmt='.',color='g', label=\"Validation\") ax1.set_ylabel('Mean Arctangent Absolute Percentage Error', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=14) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.legend(loc='best', fontsize=14, numpoints=1) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) #plt.ylim(0,0.3)#np.array([all_maape_training,all_maape_validation]).max()+0.005) #plt.xlim(0.1,1) plt.tight_layout()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a95ab",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### False positives and negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846b50d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "def get_false_pos_neg(cutoff, metric_array, predicted_array):\n",
    "```\n",
    "-------------\n",
    "false positive\n",
    "\n",
    "```\n",
    "indices_actual_above_cutoff = np.where(cutoff <= metric_array)[0]\n",
    "indices_predicted_under_cutoff = np.where(predicted_array < cutoff)[0]\n",
    "intersection_false_positive = np.intersect1d(indices_actual_above_cutoff, indices_predicted_under_cutoff)\n",
    "false_positive = len(intersection_false_positive)/len(indices_actual_above_cutoff)\n",
    "\n",
    "#false_negative\n",
    "indices_actual_under_cutoff = np.where(cutoff >= metric_array)[0]\n",
    "indices_predicted_above_cutoff = np.where(predicted_array > cutoff)[0]\n",
    "intersection_false_negative = np.intersect1d(indices_actual_under_cutoff, indices_predicted_above_cutoff)\n",
    "false_negative = len(intersection_false_negative)/len(indices_actual_under_cutoff)\n",
    "\n",
    "correct = 1-false_positive-false_negative\n",
    "return false_positive, false_negative, correct\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67541567",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "x_axis = [] all_false_positive_ratios_val = [] all_false_negative_ratios_val = [] correct_val = []\n",
    "\n",
    "if (len(Y_train) != len(predicted_training) or len(Y_validate) != len(predicted_validation)): raise ValueError(\"The arrays of actual values and predictions did not have the same shape!\")\n",
    "\n",
    "cutoff = decimal.Decimal('0.1') while (cutoff <= decimal.Decimal('0.9')): false_positive_ratio_val, false_negative_ratio_val, correct_ratio_val = get_false_pos_neg(cutoff, Y_validate, predicted_validation) all_false_positive_ratios_val.append(false_positive_ratio_val) all_false_negative_ratios_val.append(false_negative_ratio_val) x_axis.append(cutoff)\n",
    "    \n",
    "```\n",
    "----------\n",
    "```\n",
    "cutoff += decimal.Decimal('0.01')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb8b82",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6291e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') #l1 = ax1.errorbar(x_axis, all_mape_training, all_errorbars_mape_training, color='cornflowerblue', label=\"Training\") l2 = ax1.errorbar(x_axis, all_mape_validation, all_errorbars_mape_validation, color='g', label=\"Validation\")\n",
    "\n",
    "#l3 = ax1.scatter(x_axis, all_mape_training, s = 0.1np.array(datapoints_training), color='r') l4 = ax1.scatter(x_axis, all_mape_validation, s = 0.5np.array(datapoints_validation), color='r')\n",
    "\n",
    "#Colour legend #fig.legend(handles=[l1,l2], loc=[0.1111,0.13], numpoints=1)\n",
    "\n",
    "#Size legend handles, labels = l4.legend_elements(prop=\"sizes\", num = [50,100,250,500,1000], alpha = 0.5, color='red') ax1.legend(handles, labels, loc=\"best\", title=\"#Datapoints\", numpoints = 1, labelspacing=1)\n",
    "\n",
    "ax1.set_ylabel('Mean Absolute Percentage Error', fontsize=18) ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=15) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.grid() plt.ylim(0,0.6)#np.array([all_mean_deviations_training,all_mean_deviations_validation]).max()+0.005) plt.xlim(0,1) plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23309a78",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "fig, ax1 = plt.subplots() fig.patch.set_facecolor('white') l1 = ax1.scatter(x_axis, all_false_negative_ratios_val, marker='x', s=100, color='cornflowerblue', label=\"Negative\", linewidth=3) l2 = ax1.scatter(x_axis, all_false_positive_ratios_val, marker='x', s=100, color='g', label=\"Positive\", linewidth=3) #ax1.set_ylabel('False count ratio', fontsize=18) #ax1.set_xlabel('3D MSD fourth-root', fontsize=18) plt.tick_params(axis='both', which='major', labelsize=25) plt.tick_params(axis='both', which='minor', labelsize=8) ax1.legend(loc='best', fontsize=14, scatterpoints=1) ax1.xaxis.tick_top() ax1.yaxis.tick_right() plt.grid() #ax1.xaxis.set_ticks(np.arange(0, 1.1, 0.1)) #ax1.yaxis.set_ticks(np.arange(0, 1.1, 0.1)) plt.ylim(0.025,0.23)#np.array([all_false_negative_ratios_val, all_false_positive_ratios_val]).max()+0.005) plt.xlim(0.35,0.55) plt.tight_layout()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2679d89",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Occlusion Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427cecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.605558Z",
     "start_time": "2022-05-15T11:12:25.431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def iter_occlusion(image, size=1):\n",
    "    #this is where you choose the size of the patch\n",
    "    #occlusion = np.full((size, size, 1), [0.5], np.float32)\n",
    "    occlusion_center = np.full((size, size, 1), [0.5], np.float32)\n",
    "    occlusion_padding = size \n",
    "    # print('padding...')\n",
    "    #np.pad(input array, Number of values padded to the edges of each axis, pads with constant value)\n",
    "    image_padded = np.pad(image, ( \\\n",
    "                        (occlusion_padding, occlusion_padding), (occlusion_padding, occlusion_padding), (0, 0) \\\n",
    "                        ), 'constant', constant_values = 0.0)\n",
    "\n",
    "    for y in range(occlusion_padding, image.shape[0] + occlusion_padding, size): #range(start, stop, step)\n",
    "\n",
    "        for x in range(occlusion_padding, image.shape[1] + occlusion_padding, size): #range(start, stop, step)\n",
    "            tmp = image_padded.copy()\n",
    "            \n",
    "            #: means values between these values, y - occlusion_padding:y = y \n",
    "            #for size = 10, x - occlusion_padding:y = 20\n",
    "            #tmp[y - occlusion_padding:y + occlusion_center.shape[0] + occlusion_padding - 4*occlusion_size, \\\n",
    "            #    x - occlusion_padding:x + occlusion_center.shape[1] + occlusion_padding - 4*occlusion_size] \\\n",
    "            #    = occlusion\n",
    "\n",
    "            tmp[y:y + occlusion_center.shape[0], x:x + occlusion_center.shape[1]] = occlusion_center\n",
    "            \n",
    "            \n",
    "            yield x - occlusion_padding, y - occlusion_padding, \\\n",
    "                  tmp[occlusion_padding:tmp.shape[0] - occlusion_padding, occlusion_padding:tmp.shape[1] \\\n",
    "                      - occlusion_padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a212be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.606825Z",
     "start_time": "2022-05-15T11:12:25.433Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "deviation = np.abs(Y_validate - predicted_metric_validation[:,0])\n",
    "max_deviation_slice = np.where(deviation == np.max(deviation))[0][0]\n",
    "min_deviation_slice = np.where(deviation == np.min(deviation))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82518d49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.608284Z",
     "start_time": "2022-05-15T11:12:25.436Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction_max_bkground = model.predict((X_validate[max_deviation_slice]+300).reshape(1,512,512,4))\n",
    "prediction_min_bkground = model.predict((X_validate[min_deviation_slice]+300).reshape(1,512,512,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7298a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "USE THIS TO WRITE OUT TO A FILE WHAT THE ACTUAL VALUES AND PREDICTIONS ARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5aad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.609541Z",
     "start_time": "2022-05-15T11:12:25.439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "occlusion = \"Max deviation slice: \"+str(max_deviation_slice)+\". Actual value: \"+str(Y_validate[max_deviation_slice])+\". \\\n",
    "Prediction: \"+str(predicted_metric_validation[max_deviation_slice][0])+\". Prediction background \\\n",
    "300: \"+str(prediction_max_bkground[0][0])+\". \\nMin deviation Slice: \"+str(min_deviation_slice)+\" Actual value: \\\n",
    "\"+str(Y_validate[min_deviation_slice])+\". Prediction: \"+str(predicted_metric_validation[min_deviation_slice][0])+\". Prediction\\\n",
    " background 300: \"+str(prediction_min_bkground[0][0])+\".\"\n",
    "with open(output_folder+'occlusion.txt', 'w') as f:\n",
    "    f.write(occlusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8561e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`for i in range(1000): print(str(i) + \"--> \" + str(Y_validate[i]) + \" \" + str(predicted_metric_validation[i][0]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3ba7c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the image to be occluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec3612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.610803Z",
     "start_time": "2022-05-15T11:12:25.442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = max_deviation_slice\n",
    "data = X_validate[i]+300\n",
    "\n",
    "# Define the corresponding data\n",
    "actual_metric = Y_validate[i]\n",
    "predicted_metric = predicted_metric_validation[i][0]\n",
    "\n",
    "print('Actual value of metric: ', actual_metric)\n",
    "print('Unoccluded prediction from predicted_metric_validation: ', predicted_metric)\n",
    "\n",
    "# input tensor for model.predict\n",
    "#for multiple_patient_image_array\n",
    "inp = data.reshape(1, 512, 512, 4)\n",
    "\n",
    "prediction_i = model.predict(inp)\n",
    "print('Unoccluded prediction from predicting on X_validate[i]+300: ', prediction_i[0,0])\n",
    "\n",
    "# occlusion data\n",
    "img_size = inp.shape[1]\n",
    "occlusion_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d7e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.612110Z",
     "start_time": "2022-05-15T11:12:25.444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data[:,:,0].astype(np.float64), cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.savefig(output_folder+\"i=\"+str(i)+\"_max_deviation_300bkg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfe54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.613370Z",
     "start_time": "2022-05-15T11:12:25.446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('occluding...')\n",
    "\n",
    "heatmap = np.zeros((512, 512), np.float64)\n",
    "\n",
    "start=time.time()\n",
    "for n, (x, y, img_float) in enumerate(iter_occlusion(data, size=occlusion_size)):\n",
    "    \n",
    "    X = img_float.reshape(1, 512, 512, 4)\n",
    "    out = model.predict(X)\n",
    "    print('{}: Occluded prediction: {}'.format(n, out[0,0]))\n",
    "    print('x {} - {} | y {} - {}'.format(x, x + occlusion_size, y, y + occlusion_size))\n",
    "\n",
    "    heatmap[y:y + occlusion_size, x:x + occlusion_size] = out[0,0]\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"These occlusions took: \" + str(stop-start) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66078c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.614562Z",
     "start_time": "2022-05-15T11:12:25.449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1, aspect='equal')\n",
    "hm = ax1.imshow(heatmap, cmap='viridis', origin='lower')\n",
    "ax1.set_xlabel('x', fontsize=18)\n",
    "ax1.set_ylabel('y', fontsize=18)\n",
    "\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "cbar1 = plt.colorbar(hm, cax=cax1, format = '%1.6f') #CHECK THIS FOR EACH PLOT\n",
    "cbar1.ax.set_ylabel(metric)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(output_folder + 'occlusion_map_'+metric+'_' + 'i:'+str(i) + '_' + organ + '_max_deviation_300bkg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b9eb1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create a heat map of window importance by subtracting overall predicted from predicted for each square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7fdcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.615797Z",
     "start_time": "2022-05-15T11:12:25.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ground_truth = predicted_metric_validation[i]\n",
    "ground_truth = prediction_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f6f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.617070Z",
     "start_time": "2022-05-15T11:12:25.456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "#occlusion_data = np.rot90(heatmap-predicted_metric_validation[i], k=1, axes=(0,1))\n",
    "ax1 = plt.subplot(1, 2, 1, aspect='equal')\n",
    "maxDiff = np.amax(abs(heatmap-ground_truth))\n",
    "\n",
    "hm = ax1.imshow(heatmap-ground_truth, cmap='Spectral', origin='lower', vmin = -maxDiff, vmax =maxDiff)\n",
    "\n",
    "ax1.set_xlabel('x', fontsize=18)\n",
    "ax1.set_ylabel('y', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "cbar1 = plt.colorbar(hm, cax=cax1)\n",
    "cbar1.ax.set_ylabel('Change in '+metric+' Due to Occlusion', fontsize =18)\n",
    "cbar1.ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(output_folder + 'occlusion_map_'+metric+'_change_' + 'i:'+str(i) + '_' + organ + '_max_deviation_300bkg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff521cb0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Now the min deviation slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b9f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.618279Z",
     "start_time": "2022-05-15T11:12:25.461Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the image to be occluded\n",
    "i = min_deviation_slice\n",
    "data = X_validate[i]+300\n",
    "\n",
    "# Define the corresponding data\n",
    "actual_metric = Y_validate[i]\n",
    "predicted_metric = predicted_metric_validation[i][0]\n",
    "\n",
    "print('Actual value of metric: ', actual_metric)\n",
    "print('Unoccluded prediction from predicted_metric_validation: ', predicted_metric)\n",
    "\n",
    "# input tensor for model.predict\n",
    "#for multiple_patient_image_array\n",
    "inp = data.reshape(1, 512, 512, 4)\n",
    "\n",
    "prediction_i = model.predict(inp)\n",
    "print('Unoccluded prediction from predicting on X_validate[i]+300: ', prediction_i[0,0])\n",
    "\n",
    "# occlusion data\n",
    "img_size = inp.shape[1]\n",
    "occlusion_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4a2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.619705Z",
     "start_time": "2022-05-15T11:12:25.463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data[:,:,0].astype(np.float64), cmap='magma')\n",
    "plt.colorbar()\n",
    "plt.savefig(output_folder+\"i=\"+str(i)+\"_min_deviation_300bkg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45224b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.621899Z",
     "start_time": "2022-05-15T11:12:25.465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('occluding...')\n",
    "\n",
    "heatmap = np.zeros((512, 512), np.float64)\n",
    "\n",
    "start=time.time()\n",
    "for n, (x, y, img_float) in enumerate(iter_occlusion(data, size=occlusion_size)):\n",
    "    \n",
    "    X = img_float.reshape(1, 512, 512, 4)\n",
    "    out = model.predict(X)\n",
    "    print('{}: Occluded prediction: {}'.format(n, out[0,0]))\n",
    "    print('x {} - {} | y {} - {}'.format(x, x + occlusion_size, y, y + occlusion_size))\n",
    "\n",
    "    heatmap[y:y + occlusion_size, x:x + occlusion_size] = out[0,0]\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(\"These occlusions took: \" + str(stop-start) + \" seconds.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec799a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.623107Z",
     "start_time": "2022-05-15T11:12:25.468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1, aspect='equal')\n",
    "hm = ax1.imshow(heatmap, cmap='viridis', origin='lower')\n",
    "ax1.set_xlabel('x', fontsize=18)\n",
    "ax1.set_ylabel('y', fontsize=18)\n",
    "\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "cbar1 = plt.colorbar(hm, cax=cax1, format = '%1.6f') #CHECK THIS FOR EACH PLOT\n",
    "cbar1.ax.set_ylabel(metric)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(output_folder + 'occlusion_map_'+metric+'_' + 'i:'+str(i) + '_' + organ + '_min_deviation_300bkg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05550c6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.624245Z",
     "start_time": "2022-05-15T11:12:25.470Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ground_truth = predicted_metric_validation[i]\n",
    "ground_truth = prediction_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405481f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T11:12:25.818720Z",
     "start_time": "2022-05-15T11:12:25.727219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "#occlusion_data = np.rot90(heatmap-predicted_metric_validation[i], k=1, axes=(0,1))\n",
    "ax1 = plt.subplot(1, 2, 1, aspect='equal')\n",
    "maxDiff = np.amax(abs(heatmap-ground_truth))\n",
    "\n",
    "hm = ax1.imshow(heatmap-ground_truth, cmap='Spectral', origin='lower', vmin = -maxDiff, vmax =maxDiff)\n",
    "\n",
    "ax1.set_xlabel('x', fontsize=18)\n",
    "ax1.set_ylabel('y', fontsize=18)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "cbar1 = plt.colorbar(hm, cax=cax1)\n",
    "cbar1.ax.set_ylabel('Change in '+metric+' Due to Occlusion', fontsize =18)\n",
    "cbar1.ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(output_folder + 'occlusion_map_'+metric+'_change_' + 'i:'+str(i) + '_' + organ + '_min_deviation_300bkg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f465d",
   "metadata": {},
   "source": [
    "## DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1454f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
